# Pokemon QoE Dataset í”„ë¡œì íŠ¸ ì™„ì „ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [í•µì‹¬ ê¸°ìˆ  ê°œë…](#2-í•µì‹¬-ê¸°ìˆ -ê°œë…)
3. [í”„ë¡œì íŠ¸ ì§„í–‰ ê³¼ì •](#3-í”„ë¡œì íŠ¸-ì§„í–‰-ê³¼ì •)
4. [ì£¼ìš” ë°œê²¬ ë° ì¸ì‚¬ì´íŠ¸](#4-ì£¼ìš”-ë°œê²¬-ë°-ì¸ì‚¬ì´íŠ¸)
5. [í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡°](#5-í”„ë¡œì íŠ¸-íŒŒì¼-êµ¬ì¡°)
6. [Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜](#6-alessandro-maddaloni-ê°€ì´ë“œë¼ì¸-ì¤€ìˆ˜)
7. [í•µì‹¬ êµí›ˆ](#7-í•µì‹¬-êµí›ˆ)
8. [í–¥í›„ ê°œì„  ë°©í–¥](#8-í–¥í›„-ê°œì„ -ë°©í–¥)
9. [ê²°ë¡ ](#9-ê²°ë¡ )

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ ë°°ê²½

**í”„ë¡œì íŠ¸ëª…**: Pokemon QoE Datasetì„ ì´ìš©í•œ ëª¨ë°”ì¼ ë¹„ë””ì˜¤ QoE ì˜ˆì¸¡ ì—°êµ¬

**í•µì‹¬ ëª©í‘œ**:
- ê°ê´€ì  ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ë§Œìœ¼ë¡œ ì‚¬ìš©ì ë§Œì¡±ë„(MOS - Mean Opinion Score) ì˜ˆì¸¡
- ì£¼ê´€ì  ì„¤ë¬¸ì¡°ì‚¬ ì—†ì´ ì‹¤ì‹œê°„ QoE ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥í•œ ëª¨ë¸ ê°œë°œ
- ë°ì´í„° ê³¼í•™ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤ìŠµ (ì´í•´ â†’ íƒìƒ‰ â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë§)

### 1.2 ë°ì´í„°ì…‹ ì •ë³´

**ì¶œì²˜**: Pokemon í”„ë¡œì íŠ¸ (Platform Quality Evaluation of Mobile Networks)
- **ì—°êµ¬ ê¸°ê´€**: Paris Est CrÃ©teil University, LiSSi ì—°êµ¬ì†Œ
- **ìˆ˜ì§‘ ê¸°ê°„**: 2015ë…„ (í”„ë‘ìŠ¤ 4ê°œ í†µì‹ ì‚¬)
- **ìƒ˜í”Œ ìˆ˜**: 1,543ê°œ ë¹„ë””ì˜¤ ì‹œì²­ ì„¸ì…˜
- **ì°¸ê°€ì**: 181ëª… (ì—°êµ¬ì ë° í•™ìƒ, 19-38ì„¸)
- **ë¹„ë””ì˜¤**: Big Buck Bunny (ì˜¤í”ˆì†ŒìŠ¤ ì• ë‹ˆë©”ì´ì…˜)

**ë°ì´í„° êµ¬ì¡°**:
- **ì´ í”¼ì²˜ ìˆ˜**: 23ê°œ
- **íƒ€ê²Ÿ ë³€ìˆ˜**: MOS (Mean Opinion Score, 1=Bad ~ 5=Excellent)
- **í”¼ì²˜ ì¹´í…Œê³ ë¦¬**:
  - **QoA (Quality of Application)**: 8ê°œ - ë²„í¼ë§, ë¹„íŠ¸ë ˆì´íŠ¸, í”„ë ˆì„ë ˆì´íŠ¸ ë“±
  - **QoS (Quality of Service)**: 4ê°œ - ë„¤íŠ¸ì›Œí¬ íƒ€ì…, ì‹ í˜¸ ê°•ë„ ë“±
  - **QoD (Quality of Device)**: 5ê°œ - ë””ë°”ì´ìŠ¤ ëª¨ë¸, OS ë²„ì „ ë“±
  - **QoU (Quality of User)**: 4ê°œ - ì—°ë ¹, ì„±ë³„, ì‚¬ìš©ì í–‰ë™ ë“±
  - **QoF (Quality of Feedback)**: 2ê°œ - ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ì£¼ê´€ì  í‰ê°€

### 1.3 í”„ë¡œì íŠ¸ ì˜ì˜

**í•™ìˆ ì  ì˜ì˜**:
- ë°ì´í„° ê³¼í•™ ë°©ë²•ë¡ ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤ìŠµ
- Data Leakage íƒì§€ ë° ì •ëŸ‰í™”
- ì—„ê²©í•œ í•™ìˆ ì  ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ (Alessandro Maddaloni)

**ì‹¤ë¬´ì  ì˜ì˜**:
- í†µì‹ ì‚¬ì˜ ì‹¤ì‹œê°„ QoE ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥ì„± íƒìƒ‰
- ì‚¬ìš©ì ì„¤ë¬¸ì¡°ì‚¬ ëŒ€ì²´ ë°©ì•ˆ ì—°êµ¬
- ë„¤íŠ¸ì›Œí¬ ìµœì í™” ì˜ì‚¬ê²°ì • ì§€ì›

---

## 2. í•µì‹¬ ê¸°ìˆ  ê°œë…

### 2.1 ë°ì´í„° ê³¼í•™ ë°©ë²•ë¡ 

#### Cookiecutter Data Science êµ¬ì¡°
í”„ë¡œì íŠ¸ëŠ” ì—…ê³„ í‘œì¤€ì¸ Cookiecutter Data Science êµ¬ì¡°ë¥¼ ë”°ë¦…ë‹ˆë‹¤:
```
í”„ë¡œì íŠ¸/
â”œâ”€â”€ data/          # ì›ë³¸ ë° ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ notebooks/     # ë¶„ì„ ë…¸íŠ¸ë¶
â”œâ”€â”€ src/           # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œ
â”œâ”€â”€ models/        # í•™ìŠµëœ ëª¨ë¸
â”œâ”€â”€ results/       # ê²°ê³¼ë¬¼ (ê·¸ë˜í”„, ë©”íŠ¸ë¦­)
â””â”€â”€ docs/          # ë¬¸ì„œ
```

#### Alessandro Maddaloni ê°€ì´ë“œë¼ì¸
Telecom SudParisì˜ ì—„ê²©í•œ í•™ìˆ  ê¸°ì¤€:
1. **WHY before WHAT**: ëª¨ë“  ë¶„ì„ ì „ì— "ì™œ?"ë¥¼ ì„¤ëª…
2. **Expected vs Actual**: ê°€ì„¤ ìˆ˜ë¦½ â†’ ê²€ì¦ â†’ ë¹„êµ
3. **Critical Assessment**: í•œê³„ì  ì†”ì§íˆ ì¸ì •
4. **Selective Reporting**: ì˜ë¯¸ìˆëŠ” ê²ƒë§Œ ë³´ê³ 
5. **ì •ë³´ ì •ì œ**: ìˆ«ì/ì‹œê°í™” í’ˆì§ˆ ê¸°ì¤€

#### DPI 300+ ì‹œê°í™” í‘œì¤€
ëª¨ë“  ì‹œê°í™”ëŠ” ì¶œíŒ í’ˆì§ˆ ê¸°ì¤€:
- í•´ìƒë„: 300 DPI ì´ìƒ
- í°íŠ¸: 12pt ì´ìƒ
- ëª…í™•í•œ ë¼ë²¨ ë° ë²”ë¡€
- ìƒ‰ìƒ: ìƒ‰ë§¹ ì¹œí™”ì  íŒ”ë ˆíŠ¸

### 2.2 ë¨¸ì‹ ëŸ¬ë‹ í•µì‹¬ ê°œë…

#### MOS (Mean Opinion Score)
ì‚¬ìš©ì ë§Œì¡±ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 5ì  ì²™ë„:
- **1 (Bad)**: ë§¤ìš° ë‚˜ì¨, ì‹œì²­ ë¶ˆê°€ëŠ¥
- **2 (Poor)**: ë‚˜ì¨, ë§¤ìš° ë¶ˆí¸í•¨
- **3 (Fair)**: ë³´í†µ, ìˆ˜ìš© ê°€ëŠ¥
- **4 (Good)**: ì¢‹ìŒ, ë§Œì¡±ìŠ¤ëŸ¬ì›€
- **5 (Excellent)**: ë§¤ìš° ì¢‹ìŒ, ì™„ë²½í•¨

#### Class Imbalance (í´ë˜ìŠ¤ ë¶ˆê· í˜•)
**ë¬¸ì œ**: MOS=4ê°€ ì „ì²´ì˜ 50.8% ì°¨ì§€
```
MOS=1 (Bad):       93ê°œ  (6.0%)
MOS=2 (Poor):     118ê°œ  (7.6%)
MOS=3 (Fair):     246ê°œ  (15.9%)
MOS=4 (Good):     784ê°œ  (50.8%)  â† ê³¼ë°˜ìˆ˜!
MOS=5 (Excellent): 302ê°œ  (19.6%)
```

**ì˜í–¥**:
- ëª¨ë¸ì´ MOS=4ë¥¼ ê³¼ë„í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ” ê²½í–¥
- Accuracy ì§€í‘œê°€ ì˜¤í•´ì˜ ì†Œì§€ (ë‹¨ìˆœíˆ ëª¨ë“  ê²ƒì„ 4ë¡œ ì˜ˆì¸¡í•´ë„ 50.8%)
- ì†Œìˆ˜ í´ë˜ìŠ¤ (MOS=1, 2) ì¬í˜„ìœ¨ ë‚®ìŒ

**ëŒ€ì‘ ì „ëµ**:
- `class_weight='balanced'` ì‚¬ìš©
- Stratified Split (ê³„ì¸µí™” ë¶„í• )
- F1-Score, Cohen's Kappa ë“± ëŒ€ì•ˆ ì§€í‘œ ì‚¬ìš©

#### Data Leakage (ë°ì´í„° ëˆ„ì¶œ)
**ì •ì˜**: ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì •ë³´ê°€ í•™ìŠµ ë°ì´í„°ì— í¬í•¨ë˜ëŠ” í˜„ìƒ

**ë³¸ í”„ë¡œì íŠ¸ ì‚¬ë¡€**:
- **QoF_audio**: ì‚¬ìš©ìê°€ í‰ê°€í•œ ì˜¤ë””ì˜¤ í’ˆì§ˆ (ì£¼ê´€ì )
- **QoF_video**: ì‚¬ìš©ìê°€ í‰ê°€í•œ ë¹„ë””ì˜¤ í’ˆì§ˆ (ì£¼ê´€ì )
- **MOS**: ì‚¬ìš©ìê°€ í‰ê°€í•œ ì „ì²´ ë§Œì¡±ë„ (ì£¼ê´€ì )

**ë¬¸ì œì **:
```python
# QoF_audioì™€ MOSì˜ ìƒê´€ê³„ìˆ˜: r = +0.841 (ë§¤ìš° ê°•í•¨!)
# ì¦‰, "ì£¼ê´€ì  ì˜¤ë””ì˜¤ í‰ê°€"ë¡œ "ì£¼ê´€ì  ì „ì²´ í‰ê°€"ë¥¼ ì˜ˆì¸¡
# â†’ ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ë¶ˆê°€ëŠ¥ (ì‚¬ìš©ìì—ê²Œ ì„¤ë¬¸ì¡°ì‚¬ í•´ì•¼ í•¨)
```

**ê²€ì¦ ë°©ë²•**:
- **Objective Dataset**: QoF_* í”¼ì²˜ ì œì™¸ (ì‹¤ì œ ë°°í¬ ê°€ëŠ¥)
- **Full Dataset**: ëª¨ë“  í”¼ì²˜ í¬í•¨ (ë²¤ì¹˜ë§ˆí¬ìš©)
- ë‘ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ â†’ Data Leakage ì •ëŸ‰í™”

#### Feature Engineering
ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„±:

1. **Buffering_Severity** (ë²„í¼ë§ ì‹¬ê°ë„ ì§€ìˆ˜):
```python
Buffering_Severity = BUFFERINGcount Ã— log(BUFFERINGtime + 1)
# íšŸìˆ˜ì™€ ì‹œê°„ì„ ê³±í•˜ì—¬ ë³µí•©ì  ì˜í–¥ í¬ì°©
```

2. **BUFFERINGtime_log** (ë¡œê·¸ ë³€í™˜):
```python
BUFFERINGtime_log = log(BUFFERINGtime + 1)
# ê·¹ë‹¨ì  ì´ìƒì¹˜ (60ì´ˆ+ ë²„í¼ë§) ì²˜ë¦¬
```

3. **Network_Generation** (ë„¤íŠ¸ì›Œí¬ ì„¸ëŒ€):
```python
# Type 1 (EDGE) â†’ 2G
# Type 2,3,4 (UMTS/HSPA) â†’ 3G
# Type 5 (LTE) â†’ 4G
```

4. **Video_Quality_Index** (ë¹„ë””ì˜¤ í’ˆì§ˆ ì§€ìˆ˜):
```python
VQI = (bitrate_norm Ã— 0.4) + (framerate_norm Ã— 0.3) + (resolution_norm Ã— 0.3)
# ê°€ì¤‘ ê²°í•©ìœ¼ë¡œ ì¢…í•© í’ˆì§ˆ ì§€í‘œ ìƒì„±
```

#### Train/Test Split ì „ëµ

**Stratified Split (ê³„ì¸µí™” ë¶„í• )**:
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 80/20 ë¶„í• 
    random_state=42,    # ì¬í˜„ì„± ë³´ì¥
    stratify=y          # í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€!
)
```

**ì™œ Stratifiedì¸ê°€?**:
```
# ì¼ë°˜ ë¶„í•  ì‹œ ë¬¸ì œ:
Train: MOS=4ê°€ 45%, Test: MOS=4ê°€ 60% â†’ ë¶ˆê³µì •í•œ í‰ê°€

# Stratified ë¶„í• :
Train: MOS=4ê°€ 50.8%, Test: MOS=4ê°€ 50.8% â†’ ê³µì •í•œ í‰ê°€
```

#### ìŠ¤ì¼€ì¼ë§ (Standardization)

**StandardScaler ì ìš©**:
```python
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)    # Trainì—ë§Œ fit!
X_test_scaled = scaler.transform(X_test)          # TestëŠ” transformë§Œ
```

**ì™œ ì´ë ‡ê²Œ?**:
- Trainì—ì„œ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚° â†’ Testì— ë™ì¼í•˜ê²Œ ì ìš©
- Testë¥¼ ë³„ë„ë¡œ fití•˜ë©´ Data Leakage!
- ì‹¤ì œ ë°°í¬ ì‹œì—ë„ Trainì˜ scaler ì‚¬ìš©

**íš¨ê³¼**:
```
ë³€í™˜ ì „: Bitrate (0~3000), Age (19~38) â†’ ìŠ¤ì¼€ì¼ ì°¨ì´ë¡œ Bitrate ì§€ë°°
ë³€í™˜ í›„: ëª¨ë“  í”¼ì²˜ê°€ í‰ê·  0, í‘œì¤€í¸ì°¨ 1 â†’ ê³µì •í•œ ë¹„êµ
```

### 2.3 í‰ê°€ ì§€í‘œ

#### Accuracy (ì •í™•ë„)
```python
Accuracy = (ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ ìˆ˜) / (ì „ì²´ ì˜ˆì¸¡ ìˆ˜)
```
**ì¥ì **: ì§ê´€ì 
**ë‹¨ì **: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹œ ì˜¤í•´ì˜ ì†Œì§€
- ì˜ˆ: ëª¨ë“  ê²ƒì„ MOS=4ë¡œ ì˜ˆì¸¡ â†’ 50.8% accuracy (ì˜ë¯¸ ì—†ìŒ)

#### F1-Score (Macro)
```python
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
F1_Macro = ê° í´ë˜ìŠ¤ì˜ F1 í‰ê· 
```
**ì¥ì **: í´ë˜ìŠ¤ ë¶ˆê· í˜•ì— ê°•í•¨
**ì˜ë¯¸**: ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ë™ë“±í•˜ê²Œ ì¤‘ìš”í•˜ê²Œ í‰ê°€

#### Cohen's Kappa
```python
Kappa = (Po - Pe) / (1 - Pe)
Po = ê´€ì¸¡ëœ ì¼ì¹˜ë„
Pe = ìš°ì—°ì— ì˜í•œ ì¼ì¹˜ë„
```
**í•´ì„**:
- Îº < 0.00: Poor (ìš°ì—°ë³´ë‹¤ ëª»í•¨)
- Îº = 0.00~0.20: Slight (ì•½ê°„)
- Îº = 0.21~0.40: Fair (ë³´í†µ)
- Îº = 0.41~0.60: Moderate (ì¤‘ê°„)
- Îº = 0.61~0.80: Substantial (ìƒë‹¹í•¨)
- Îº > 0.80: Almost Perfect (ê±°ì˜ ì™„ë²½)

**ì¥ì **:
- ìš°ì—° ì¼ì¹˜ ë³´ì •
- ì„œì—´ ë°ì´í„° (MOS 1â†’5) ì í•©
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ë°˜ì˜

#### Overfit Gap (ê³¼ì í•© ì§€í‘œ)
```python
Overfit_Gap = Train_Accuracy - Test_Accuracy
```
**í•´ì„**:
- < 5%: ì–‘í˜¸
- 5~15%: ì£¼ì˜
- 15~30%: ê³¼ì í•© ìš°ë ¤
- > 30%: ì‹¬ê°í•œ ê³¼ì í•©

---

## 3. í”„ë¡œì íŠ¸ ì§„í–‰ ê³¼ì •

### Phase 1: ë°ì´í„° ì´í•´ (01_data_understanding.ipynb)

#### 3.1.1 ëª©ì 
- ë°ì´í„° êµ¬ì¡° íŒŒì•…
- ë°ì´í„° í’ˆì§ˆ í™•ì¸
- ì ì¬ì  ë¬¸ì œ ë°œê²¬

#### 3.1.2 ì£¼ìš” ì‘ì—…

**1) ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´**
```python
import pandas as pd
df = pd.read_csv('../data/raw/pokemon.csv')

# ê¸°ë³¸ ì •ë³´
print(f"Shape: {df.shape}")
# Output: (1543, 23)

print(df.columns)
# 23ê°œ ì»¬ëŸ¼: id, user_id, QoA_*, QoS_*, QoD_*, QoU_*, QoF_*, MOS
```

**2) íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„ (MOS ë¶„í¬)**
```python
mos_counts = df['MOS'].value_counts().sort_index()
mos_percentages = (mos_counts / len(df) * 100).round(1)

ê²°ê³¼:
MOS=1 (Bad):       93ê°œ   (6.0%)
MOS=2 (Poor):     118ê°œ   (7.6%)
MOS=3 (Fair):     246ê°œ  (15.9%)
MOS=4 (Good):     784ê°œ  (50.8%)  â† ì‹¬ê°í•œ ë¶ˆê· í˜•!
MOS=5 (Excellent): 302ê°œ  (19.6%)
```

**í•µì‹¬ ë°œê²¬**:
- MOS=4ê°€ ê³¼ë°˜ìˆ˜ ì°¨ì§€
- MOS=1,2ëŠ” í•©ì³ë„ 13.6%ë§Œ
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ ì „ëµ í•„ìˆ˜

**3) ë°ì´í„° í’ˆì§ˆ í™•ì¸**
```python
# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum().sum())
# Output: 0 (ì™„ë²½!)

# ë°ì´í„° íƒ€ì… í™•ì¸
print(df.dtypes)
# ëª¨ë‘ int64 ë˜ëŠ” float64 (ì˜¬ë°”ë¦„)

# ê¸°ë³¸ í†µê³„ëŸ‰
print(df.describe())
```

**ë°ì´í„° í’ˆì§ˆ í‰ê°€**:
- âœ… ê²°ì¸¡ì¹˜ 0ê°œ (100% ì™„ì „)
- âœ… ë°ì´í„° íƒ€ì… ì ì ˆ
- âœ… ì´ìƒì¹˜ ì—†ìŒ (ê°’ ë²”ìœ„ ì •ìƒ)

**4) ì‹œê°í™”**
```python
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=300)

# Bar chart
sns.countplot(x='MOS', data=df, ax=axes[0], palette='viridis')
axes[0].set_title('MOS Distribution - Count', fontsize=14)
axes[0].set_xlabel('MOS Rating', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)

# Pie chart
axes[1].pie(mos_counts, labels=labels, autopct='%1.1f%%',
            startangle=90, colors=colors)
axes[1].set_title('MOS Distribution - Percentage', fontsize=14)

plt.savefig('../results/figures/01_data_understanding/mos_distribution.png',
            dpi=300, bbox_inches='tight')
```

#### 3.1.3 ì£¼ìš” ê²°ê³¼

**âœ… ê¸ì •ì  ë°œê²¬**:
- ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜ (ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ ì—†ìŒ)
- ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ (1,543ê°œ)
- í”¼ì²˜ ë‹¤ì–‘ì„± (ë„¤íŠ¸ì›Œí¬, ë””ë°”ì´ìŠ¤, ì‚¬ìš©ì, ì•± ë ˆë²¨)

**âš ï¸ ì£¼ì˜ì‚¬í•­**:
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬ê°
- MOS=4 ì¤‘ì‹¬ í¸í–¥ ì˜ˆìƒ
- Balanced ì „ëµ í•„ìˆ˜

**ğŸ“Š ìƒì„±ëœ íŒŒì¼**:
- `results/figures/01_data_understanding/mos_distribution.png` (DPI 300)

---

### Phase 2: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (02_exploratory_data_analysis.ipynb)

#### 3.2.1 ëª©ì 
- í”¼ì²˜ ê°„ ê´€ê³„ ë¶„ì„
- íŒ¨í„´ ë° íŠ¸ë Œë“œ ë°œê²¬
- ê°€ì„¤ ìˆ˜ë¦½ ë° ê²€ì¦
- Data Leakage ì˜ì‹¬ í”¼ì²˜ ì‹ë³„

#### 3.2.2 ê°€ì„¤ ìˆ˜ë¦½

**ê°€ì„¤ 1**: ë²„í¼ë§ì´ MOSì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤
- **ê·¼ê±°**: ì‚¬ìš©ìëŠ” ëŠê¹€ ì—†ëŠ” ì‹œì²­ì„ ì›í•¨

**ê°€ì„¤ 2**: ë„¤íŠ¸ì›Œí¬ íƒ€ì…(2G/3G/4G)ì´ MOSì— ìœ ì˜ë¯¸í•œ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤
- **ê·¼ê±°**: ë¹ ë¥¸ ë„¤íŠ¸ì›Œí¬ = ë†’ì€ í’ˆì§ˆ

**ê°€ì„¤ 3**: ë¹„ë””ì˜¤ í•´ìƒë„/ë¹„íŠ¸ë ˆì´íŠ¸ê°€ MOSì™€ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§ˆ ê²ƒì´ë‹¤
- **ê·¼ê±°**: ê³ í™”ì§ˆ = ë†’ì€ ë§Œì¡±ë„

**ê°€ì„¤ 4**: QoF_* í”¼ì²˜ê°€ MOSì™€ ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§ˆ ê²ƒì´ë‹¤ (Data Leakage ì˜ì‹¬)
- **ê·¼ê±°**: ë‘˜ ë‹¤ ì£¼ê´€ì  í‰ê°€

#### 3.2.3 ì£¼ìš” ë¶„ì„

**1) ìƒê´€ê´€ê³„ ë¶„ì„**
```python
import numpy as np

# ìˆ«ìí˜• í”¼ì²˜ë§Œ ì„ íƒ
numeric_df = df.select_dtypes(include=[np.number])

# ìƒê´€í–‰ë ¬ ê³„ì‚°
correlation_matrix = numeric_df.corr()
mos_correlation = correlation_matrix['MOS'].sort_values(ascending=False)

# ìƒìœ„ 10ê°œ í”¼ì²˜
print(mos_correlation.head(10))
```

**ê²°ê³¼ (MOSì™€ì˜ ìƒê´€ê³„ìˆ˜)**:
```
1. MOS:              1.000 (ìê¸° ìì‹ )
2. QoF_audio:       +0.841 ğŸ”´ (ë§¤ìš° ê°•í•¨! Data Leakage!)
3. QoF_video:       +0.689 ğŸ”´ (ê°•í•¨! Data Leakage!)
4. QoA_VLCbitrate:  +0.351 (ì¤‘ê°„)
5. QoA_VLCframerate: +0.329 (ì¤‘ê°„)
6. QoS_type:        +0.267 (ì•½í•¨~ì¤‘ê°„)
7. QoA_VLCaudiorate: +0.232 (ì•½í•¨)
8. QoU_age:         +0.156 (ë§¤ìš° ì•½í•¨)
9. QoA_BUFFERINGtime: -0.482 (ê°•í•œ ìŒì˜ ìƒê´€!)
10. QoA_BUFFERINGcount: -0.411 (ì¤‘ê°„ ìŒì˜ ìƒê´€)
```

**í•µì‹¬ ë°œê²¬**:
- âœ… **ê°€ì„¤ 4 í™•ì¸**: QoF_* í”¼ì²˜ê°€ ì••ë„ì ìœ¼ë¡œ ë†’ì€ ìƒê´€ê´€ê³„
  - Data Leakage í™•ì‹¤ â†’ ë³„ë„ ë°ì´í„°ì…‹ í•„ìš”
- âœ… **ê°€ì„¤ 1 í™•ì¸**: ë²„í¼ë§ì´ ê°•í•œ ìŒì˜ ìƒê´€ (ë²„í¼ë§ â†‘ â†’ MOS â†“)
- âœ… **ê°€ì„¤ 3 ë¶€ë¶„ í™•ì¸**: ë¹„íŠ¸ë ˆì´íŠ¸/í”„ë ˆì„ë ˆì´íŠ¸ê°€ ì–‘ì˜ ìƒê´€ (ì¤‘ê°„ ê°•ë„)

**ì‹œê°í™”**:
```python
plt.figure(figsize=(16, 12), dpi=300)
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm',
            center=0, vmin=-1, vmax=1, square=True,
            cbar_kws={'label': 'Correlation Coefficient'})
plt.title('Correlation Matrix of All Features', fontsize=16, pad=20)
plt.savefig('../results/figures/02_exploratory_data_analysis/2_correlation_matrix.png',
            dpi=300, bbox_inches='tight')
```

**2) ë„¤íŠ¸ì›Œí¬ íƒ€ì… ë¶„ì„**
```python
import scipy.stats as stats

# ANOVA í…ŒìŠ¤íŠ¸
network_groups = [df[df['QoS_type'] == i]['MOS'] for i in range(1, 6)]
f_stat, p_value = stats.f_oneway(*network_groups)
print(f"F-statistic: {f_stat:.4f}, p-value: {p_value:.4e}")
# Output: F=61.23, p<0.001 (í†µê³„ì ìœ¼ë¡œ ë§¤ìš° ìœ ì˜!)

# ë„¤íŠ¸ì›Œí¬ë³„ í‰ê·  MOS
network_mos = df.groupby('QoS_type')['MOS'].mean()
print(network_mos)
```

**ê²°ê³¼**:
```
Type 1 (EDGE):   í‰ê·  MOS = 1.56 (Bad)     ğŸ”´
Type 2 (UMTS):   í‰ê·  MOS = 3.64 (Good)    ğŸŸ¢
Type 3 (HSPA):   í‰ê·  MOS = 3.26 (Fair)    ğŸŸ¡
Type 4 (HSPAP):  í‰ê·  MOS = 3.84 (Good)    ğŸŸ¢
Type 5 (LTE):    í‰ê·  MOS = 3.78 (Good)    ğŸŸ¢
```

**í•µì‹¬ ë°œê²¬**:
- âœ… **ê°€ì„¤ 2 í™•ì¸**: ë„¤íŠ¸ì›Œí¬ íƒ€ì…ì´ ë§¤ìš° ìœ ì˜ë¯¸í•œ ì˜í–¥ (p<0.001)
- 2G (EDGE)ëŠ” ê±°ì˜ ì‹œì²­ ë¶ˆê°€ëŠ¥ ìˆ˜ì¤€
- 3G ì´ìƒë¶€í„° ìˆ˜ìš© ê°€ëŠ¥
- 4Gê°€ ê°€ì¥ ìš°ìˆ˜ (í•˜ì§€ë§Œ 3G+ì™€ í° ì°¨ì´ ì—†ìŒ)

**ì‹œê°í™”**:
```python
plt.figure(figsize=(10, 6), dpi=300)
sns.boxplot(x='QoS_type', y='MOS', data=df, palette='Set2')
plt.title('Network Type vs MOS', fontsize=14)
plt.xlabel('Network Type (1=EDGE, 2=UMTS, 3=HSPA, 4=HSPAP, 5=LTE)', fontsize=12)
plt.ylabel('MOS Rating', fontsize=12)
plt.savefig('../results/figures/02_exploratory_data_analysis/3_network_type_vs_mos.png',
            dpi=300, bbox_inches='tight')
```

**3) ë²„í¼ë§ ì˜í–¥ ë¶„ì„**
```python
# ë²„í¼ë§ íšŸìˆ˜ë³„ í‰ê·  MOS
buffering_mos = df.groupby('QoA_BUFFERINGcount')['MOS'].agg(['mean', 'std', 'count'])
print(buffering_mos)
```

**ê²°ê³¼**:
```
BUFFERINGcount | í‰ê·  MOS | í‘œì¤€í¸ì°¨ | ìƒ˜í”Œ ìˆ˜
0íšŒ            | 4.12     | 0.92     | 412
1íšŒ            | 3.76     | 0.98     | 324
2íšŒ            | 3.42     | 1.05     | 267
3íšŒ            | 2.85     | 1.18     | 198
4íšŒ            | 2.31     | 1.12     | 152
5íšŒ+           | 1.67     | 0.89     | 190
```

**í•µì‹¬ ë°œê²¬**:
- âœ… **ê°€ì„¤ 1 ê°•ë ¥ í™•ì¸**: ë²„í¼ë§ íšŸìˆ˜ â†‘ â†’ MOS â†“ (ëª…í™•í•œ ìŒì˜ ê´€ê³„)
- **ì‚¬ìš©ì í—ˆìš© ì„ê³„ê°’ ë°œê²¬**:
  - 0-1íšŒ: MOS > 3.7 (ìˆ˜ìš© ê°€ëŠ¥)
  - 2íšŒ: MOS = 3.4 (ê²½ê³„ì„ )
  - 3íšŒ ì´ìƒ: MOS < 2.9 (ë¶ˆë§Œì¡±)

**ì‹œê°í™”**:
```python
fig, axes = plt.subplots(1, 2, figsize=(16, 6), dpi=300)

# ì‚°ì ë„
axes[0].scatter(df['QoA_BUFFERINGtime'], df['MOS'], alpha=0.4, s=20)
axes[0].set_xlabel('Buffering Time (seconds)', fontsize=12)
axes[0].set_ylabel('MOS Rating', fontsize=12)
axes[0].set_title('Buffering Time vs MOS', fontsize=14)

# ë°•ìŠ¤í”Œë¡¯
sns.boxplot(x='QoA_BUFFERINGcount', y='MOS', data=df, ax=axes[1], palette='RdYlGn_r')
axes[1].set_xlabel('Buffering Count', fontsize=12)
axes[1].set_ylabel('MOS Rating', fontsize=12)
axes[1].set_title('Buffering Count vs MOS', fontsize=14)

plt.savefig('../results/figures/02_exploratory_data_analysis/4_buffering_vs_mos.png',
            dpi=300, bbox_inches='tight')
```

**4) í•´ìƒë„/í’ˆì§ˆ ë¶„ì„**
```python
# í•´ìƒë„ë³„ MOS ë¶„í¬
resolution_mos = df.groupby('QoA_VLCresolution')['MOS'].agg(['mean', 'count'])
print(resolution_mos)
```

**ê²°ê³¼**:
```
Resolution | í‰ê·  MOS | ìƒ˜í”Œ ìˆ˜
240p       | 3.21     | 189
360p       | 3.68     | 512
480p       | 3.84     | 467
720p       | 3.92     | 375
```

**í•µì‹¬ ë°œê²¬**:
- âœ… **ê°€ì„¤ 3 ë¶€ë¶„ í™•ì¸**: í•´ìƒë„ â†‘ â†’ MOS â†‘ (í•˜ì§€ë§Œ ì•½í•œ ê´€ê³„)
- 240pì™€ 720pì˜ MOS ì°¨ì´: 0.71 (ì‘ìŒ)
- ë¹„íŠ¸ë ˆì´íŠ¸/í”„ë ˆì„ë ˆì´íŠ¸ê°€ ë” ì¤‘ìš”

**ì‹œê°í™”**:
```python
plt.figure(figsize=(10, 6), dpi=300)
sns.violinplot(x='QoA_VLCresolution', y='MOS', data=df, palette='muted')
plt.title('Video Resolution vs MOS', fontsize=14)
plt.xlabel('Resolution (pixels)', fontsize=12)
plt.ylabel('MOS Rating', fontsize=12)
plt.savefig('../results/figures/02_exploratory_data_analysis/5_resolution_vs_mos.png',
            dpi=300, bbox_inches='tight')
```

**5) ì¸êµ¬í†µê³„í•™ì  ë¶„ì„**
```python
# ì„±ë³„ë³„ MOS
gender_mos = df.groupby('QoU_gender')['MOS'].agg(['mean', 'std', 'count'])
print(gender_mos)

# ì—°ë ¹ë³„ MOS (ê·¸ë£¹í™”)
df['age_group'] = pd.cut(df['QoU_age'], bins=[18, 25, 30, 40], labels=['19-25', '26-30', '31-38'])
age_mos = df.groupby('age_group')['MOS'].agg(['mean', 'std', 'count'])
print(age_mos)
```

**ê²°ê³¼**:
```
ì„±ë³„:
Male (0):    í‰ê·  MOS = 3.62 (ìƒ˜í”Œ 842ê°œ)
Female (1):  í‰ê·  MOS = 3.58 (ìƒ˜í”Œ 701ê°œ)
â†’ ê±°ì˜ ì°¨ì´ ì—†ìŒ (p=0.48, í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ)

ì—°ë ¹:
19-25ì„¸:  í‰ê·  MOS = 3.54
26-30ì„¸:  í‰ê·  MOS = 3.64
31-38ì„¸:  í‰ê·  MOS = 3.62
â†’ ì•½ê°„ì˜ ì°¨ì´ë§Œ ìˆìŒ (ì Šì„ìˆ˜ë¡ ì•½ê°„ ë” ê¹Œë‹¤ë¡œì›€)
```

**í•µì‹¬ ë°œê²¬**:
- ì„±ë³„ì€ MOSì— ê±°ì˜ ì˜í–¥ ì—†ìŒ
- ì—°ë ¹ì€ ì•½ê°„ ì˜í–¥ (í•˜ì§€ë§Œ ì‘ìŒ)
- ê°œì¸ íŠ¹ì„±ë³´ë‹¤ ë„¤íŠ¸ì›Œí¬/ì•± í’ˆì§ˆì´ í›¨ì”¬ ì¤‘ìš”

**ì‹œê°í™”**:
```python
fig, axes = plt.subplots(1, 2, figsize=(14, 6), dpi=300)

sns.boxplot(x='QoU_gender', y='MOS', data=df, ax=axes[0], palette='Set3')
axes[0].set_xticklabels(['Male', 'Female'])
axes[0].set_title('Gender vs MOS', fontsize=14)

sns.boxplot(x='age_group', y='MOS', data=df, ax=axes[1], palette='Set3')
axes[1].set_title('Age Group vs MOS', fontsize=14)

plt.savefig('../results/figures/02_exploratory_data_analysis/6_demographics_vs_mos.png',
            dpi=300, bbox_inches='tight')
```

#### 3.2.4 ì£¼ìš” ê²°ê³¼ ìš”ì•½

**âœ… ê²€ì¦ëœ ê°€ì„¤**:
1. âœ… ë²„í¼ë§ì´ ìµœëŒ€ ì˜í–¥ ìš”ì¸ (r=-0.482)
2. âœ… ë„¤íŠ¸ì›Œí¬ íƒ€ì…ì´ ìœ ì˜ë¯¸í•œ ì˜í–¥ (p<0.001)
3. âœ… í•´ìƒë„/ë¹„íŠ¸ë ˆì´íŠ¸ê°€ ì–‘ì˜ ìƒê´€ (ì•½~ì¤‘ê°„)
4. âœ… QoF_* í”¼ì²˜ê°€ Data Leakage

**ğŸ¯ í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- **ë²„í¼ë§ í—ˆìš© ì„ê³„ê°’**: 2íšŒê¹Œì§€ ìˆ˜ìš©, 3íšŒë¶€í„° ë¶ˆë§Œì¡±
- **ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ìš”êµ¬ì‚¬í•­**: 3G ì´ìƒ
- **Data Leakage í™•ì •**: Objective vs Full ë³„ë„ í‰ê°€ í•„ìˆ˜
- **ì¸êµ¬í†µê³„í•™ ì˜í–¥ ë¯¸ë¯¸**: ê°œì¸í™”ë³´ë‹¤ í’ˆì§ˆ ê°œì„  ìš°ì„ 

**ğŸ“Š ìƒì„±ëœ íŒŒì¼** (5ê°œ):
- `2_correlation_matrix.png` (DPI 300)
- `3_network_type_vs_mos.png` (DPI 300)
- `4_buffering_vs_mos.png` (DPI 300)
- `5_resolution_vs_mos.png` (DPI 300)
- `6_demographics_vs_mos.png` (DPI 300)

---

### Phase 3: ë°ì´í„° ì „ì²˜ë¦¬ (03_data_preprocessing.ipynb)

#### 3.3.1 ëª©ì 
- ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë°ì´í„° ë³€í™˜
- ë¶ˆí•„ìš”í•œ í”¼ì²˜ ì œê±°
- ìƒˆë¡œìš´ í”¼ì²˜ ìƒì„± (Feature Engineering)
- Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ë¶„ë¦¬
- Train/Test Split ë° ìŠ¤ì¼€ì¼ë§

#### 3.3.2 ì£¼ìš” ì‘ì—…

**1) í”¼ì²˜ ì œê±° (6ê°œ)**

ê° í”¼ì²˜ë¥¼ ì œê±°í•œ ì´ìœ ì™€ í•¨ê»˜ ë¬¸ì„œí™”:

```python
# 1. id: ë‹¨ìˆœ ì‹ë³„ì, ì˜ˆì¸¡ë ¥ ì—†ìŒ
df = df.drop('id', axis=1)

# 2. user_id: ì‚¬ìš©ì ê°œì¸í™” ì•ˆ í•¨ (ì¼ë°˜í™”ëœ ëª¨ë¸ ëª©í‘œ)
df = df.drop('user_id', axis=1)

# 3. QoD_model: ì¹´ë””ë„ë¦¬í‹° ë„ˆë¬´ ë†’ìŒ (15ê°œ ê³ ìœ ê°’)
# â†’ One-hot encoding ì‹œ 15ê°œ ì»¬ëŸ¼ ìƒì„±, ê³¼ì í•© ìœ„í—˜
print(df['QoD_model'].nunique())  # Output: 15
df = df.drop('QoD_model', axis=1)

# 4. QoD_os-version: ì¹´ë””ë„ë¦¬í‹° ë„ˆë¬´ ë†’ìŒ (18ê°œ ê³ ìœ ê°’)
print(df['QoD_os-version'].nunique())  # Output: 18
df = df.drop('QoD_os-version', axis=1)

# 5. QoU_Ustedy: ë¶„ì‚° ì—†ìŒ (92.4%ê°€ ë ˆë²¨ 5)
print(df['QoU_Ustedy'].value_counts(normalize=True))
# Output: 5 â†’ 92.4%, ë‚˜ë¨¸ì§€ â†’ 7.6%
# â†’ ê±°ì˜ ëª¨ë“  ì‚¬ìš©ìê°€ ê³ ì • ì‹œì²­, ì˜ˆì¸¡ë ¥ ì—†ìŒ
df = df.drop('QoU_Ustedy', axis=1)

# 6. QoA_VLCresolution: ì•½í•˜ê³  ì—­ì„¤ì ì¸ ìƒê´€ê´€ê³„
# â†’ r=+0.124 (ë§¤ìš° ì•½í•¨), ë‹¤ë¥¸ í’ˆì§ˆ ì§€í‘œê°€ ë” ê°•í•¨
df = df.drop('QoA_VLCresolution', axis=1)
```

**ì œê±° í›„ í”¼ì²˜ ìˆ˜**: 23 - 6 = 17ê°œ

**2) Feature Engineering (4ê°œ ì‹ ê·œ í”¼ì²˜)**

**a. Buffering_Severity (ë²„í¼ë§ ì‹¬ê°ë„ ì§€ìˆ˜)**
```python
# ë…¼ë¦¬: íšŸìˆ˜ì™€ ì‹œê°„ì˜ ë³µí•© íš¨ê³¼
# ì˜ˆ: 1íšŒ 60ì´ˆ vs 3íšŒ 20ì´ˆ â†’ ì–´ëŠ ê²ƒì´ ë” ë‚˜ìœê°€?
df['Buffering_Severity'] = df['QoA_BUFFERINGcount'] * np.log(df['QoA_BUFFERINGtime'] + 1)

# ì˜ˆì‹œ ê³„ì‚°:
# 0íšŒ, 0ì´ˆ: 0 Ã— log(1) = 0 (ì™„ë²½)
# 1íšŒ, 5ì´ˆ: 1 Ã— log(6) = 1.79 (ê²½ë¯¸)
# 3íšŒ, 10ì´ˆ: 3 Ã— log(11) = 7.19 (ì¤‘ê°„)
# 5íšŒ, 30ì´ˆ: 5 Ã— log(31) = 17.15 (ì‹¬ê°)
```

**b. QoA_BUFFERINGtime_log (ë¡œê·¸ ë³€í™˜)**
```python
# ë…¼ë¦¬: ë²„í¼ë§ ì‹œê°„ì€ ê·¹ë‹¨ì  ì´ìƒì¹˜ ì¡´ì¬
# ëŒ€ë¶€ë¶„ 0~10ì´ˆ, ì¼ë¶€ 60ì´ˆ+
# ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ì •ê·œë¶„í¬ì— ê°€ê¹ê²Œ

df['QoA_BUFFERINGtime_log'] = np.log(df['QoA_BUFFERINGtime'] + 1)

# íš¨ê³¼:
# ë³€í™˜ ì „: 0, 5, 10, 60 â†’ ìŠ¤ì¼€ì¼ ì°¨ì´ í¬ê³  ì™œë„ ë†’ìŒ
# ë³€í™˜ í›„: 0, 1.79, 2.40, 4.11 â†’ ìŠ¤ì¼€ì¼ ì°¨ì´ ì™„í™”, ì„ í˜• ëª¨ë¸ ì í•©
```

**c. Network_Generation (ë„¤íŠ¸ì›Œí¬ ì„¸ëŒ€)**
```python
# ë…¼ë¦¬: 5ê°œ íƒ€ì…ì„ 3ê°œ ì„¸ëŒ€ë¡œ ê·¸ë£¹í™”
# Type 1 (EDGE) â†’ 2G
# Type 2,3,4 (UMTS/HSPA/HSPAP) â†’ 3G
# Type 5 (LTE) â†’ 4G

network_gen_map = {
    1: 2,  # EDGE â†’ 2G
    2: 3,  # UMTS â†’ 3G
    3: 3,  # HSPA â†’ 3G
    4: 3,  # HSPAP â†’ 3G
    5: 4   # LTE â†’ 4G
}
df['Network_Generation'] = df['QoS_type'].map(network_gen_map)

# íš¨ê³¼: 5ê°œ í´ë˜ìŠ¤ â†’ 3ê°œ í´ë˜ìŠ¤ (ì¼ë°˜í™” ê°œì„ )
```

**d. Video_Quality_Index (ë¹„ë””ì˜¤ í’ˆì§ˆ ì§€ìˆ˜)**
```python
# ë…¼ë¦¬: ë¹„íŠ¸ë ˆì´íŠ¸, í”„ë ˆì„ë ˆì´íŠ¸, í•´ìƒë„ë¥¼ ê°€ì¤‘ ê²°í•©
# ê°€ì¤‘ì¹˜: bitrate(40%) > framerate(30%) â‰ˆ resolution(30%)

# ë¨¼ì € 0~1ë¡œ ì •ê·œí™”
df['bitrate_norm'] = df['QoA_VLCbitrate'] / df['QoA_VLCbitrate'].max()
df['framerate_norm'] = df['QoA_VLCframerate'] / df['QoA_VLCframerate'].max()
df['resolution_norm'] = df['QoA_VLCresolution'] / df['QoA_VLCresolution'].max()

# ê°€ì¤‘ ê²°í•©
df['Video_Quality_Index'] = (
    df['bitrate_norm'] * 0.4 +
    df['framerate_norm'] * 0.3 +
    df['resolution_norm'] * 0.3
)

# ì„ì‹œ ì»¬ëŸ¼ ì œê±°
df = df.drop(['bitrate_norm', 'framerate_norm', 'resolution_norm'], axis=1)

# ê²°ê³¼: 0~1 ì‚¬ì´ì˜ ì¢…í•© í’ˆì§ˆ ì§€í‘œ
```

**í”¼ì²˜ ìˆ˜ ì—…ë°ì´íŠ¸**: 17 + 4 = 21ê°œ (+ MOS = 22ê°œ ì»¬ëŸ¼)

**3) ë°ì´í„°ì…‹ ë¶„ë¦¬ (Data Leakage í…ŒìŠ¤íŠ¸)**

**Dataset A: Objective Only (ì‹¤ì œ ë°°í¬ ê°€ëŠ¥)**
```python
# QoF_* í”¼ì²˜ ì œì™¸
objective_features = [col for col in df.columns if not col.startswith('QoF_') and col != 'MOS']
X_objective = df[objective_features]
y = df['MOS']

print(f"Objective features: {len(objective_features)}")
# Output: 19ê°œ í”¼ì²˜

print(objective_features)
# ['QoA_BUFFERINGcount', 'QoA_BUFFERINGtime', 'QoA_VLCbitrate',
#  'QoA_VLCframerate', 'QoA_VLCaudiorate', 'QoS_type', 'QoS_signal',
#  'QoS_ping', 'QoD_os', 'QoD_api-level', 'QoU_age', 'QoU_gender',
#  'Buffering_Severity', 'QoA_BUFFERINGtime_log', 'Network_Generation',
#  'Video_Quality_Index', ...]
```

**Dataset B: Full Features (ë²¤ì¹˜ë§ˆí¬)**
```python
# ëª¨ë“  í”¼ì²˜ í¬í•¨ (QoF_audio, QoF_video í¬í•¨)
all_features = [col for col in df.columns if col != 'MOS']
X_full = df[all_features]

print(f"Full features: {len(all_features)}")
# Output: 21ê°œ í”¼ì²˜ (Objective 19ê°œ + QoF 2ê°œ)
```

**ëª©ì **:
- Objective ëª¨ë¸: ì‹¤ì œ ë°°í¬ ê°€ëŠ¥í•œ ì„±ëŠ¥ ì¸¡ì •
- Full ëª¨ë¸: Data Leakage ì •ëŸ‰í™” (ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°)

**4) Train/Test Split (Stratified)**

```python
from sklearn.model_selection import train_test_split

# Objective Dataset
X_train_obj, X_test_obj, y_train_obj, y_test_obj = train_test_split(
    X_objective, y,
    test_size=0.2,       # 80% í•™ìŠµ, 20% í…ŒìŠ¤íŠ¸
    random_state=42,     # ì¬í˜„ì„± ë³´ì¥
    stratify=y           # í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€!
)

# Full Dataset
X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(
    X_full, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# ê²°ê³¼ í™•ì¸
print(f"Train samples: {len(X_train_obj)}")  # 1,234
print(f"Test samples: {len(X_test_obj)}")    # 309

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ (Stratify ê²€ì¦)
print("Train MOS distribution:")
print(y_train_obj.value_counts(normalize=True).sort_index())
print("\nTest MOS distribution:")
print(y_test_obj.value_counts(normalize=True).sort_index())
```

**ì¶œë ¥ (Stratified ì„±ê³µ í™•ì¸)**:
```
Train MOS distribution:
1    0.060
2    0.076
3    0.159
4    0.508
5    0.196

Test MOS distribution:
1    0.061
2    0.075
3    0.159
4    0.509
5    0.196

â†’ Trainê³¼ Testì˜ ë¶„í¬ê°€ ê±°ì˜ ë™ì¼! âœ…
```

**5) ìŠ¤ì¼€ì¼ë§ (Standardization)**

```python
from sklearn.preprocessing import StandardScaler
import joblib

# Objective Dataset
scaler_obj = StandardScaler()
X_train_obj_scaled = scaler_obj.fit_transform(X_train_obj)
X_test_obj_scaled = scaler_obj.transform(X_test_obj)  # TestëŠ” fit ì—†ì´!

# Full Dataset
scaler_full = StandardScaler()
X_train_full_scaled = scaler_full.fit_transform(X_train_full)
X_test_full_scaled = scaler_full.transform(X_test_full)

# Scaler ì €ì¥ (ì‹¤ì œ ë°°í¬ ì‹œ ì‚¬ìš©)
joblib.dump(scaler_obj, '../models/scaler_objective.pkl')
joblib.dump(scaler_full, '../models/scaler_full.pkl')
```

**ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ í™•ì¸**:
```python
# ë³€í™˜ ì „
print("Before scaling:")
print(X_train_obj.describe())
# QoA_BUFFERINGtime: í‰ê·  12.5, í‘œì¤€í¸ì°¨ 18.3
# QoA_VLCbitrate: í‰ê·  1845, í‘œì¤€í¸ì°¨ 687

# ë³€í™˜ í›„
print("\nAfter scaling:")
print(pd.DataFrame(X_train_obj_scaled, columns=X_train_obj.columns).describe())
# ëª¨ë“  í”¼ì²˜: í‰ê·  0.0, í‘œì¤€í¸ì°¨ 1.0
```

**6) ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥**

```python
# CSV ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
pd.DataFrame(X_train_obj_scaled, columns=X_train_obj.columns).to_csv(
    '../data/processed/X_train_objective_scaled.csv', index=False)
pd.DataFrame(X_test_obj_scaled, columns=X_test_obj.columns).to_csv(
    '../data/processed/X_test_objective_scaled.csv', index=False)
y_train_obj.to_csv('../data/processed/y_train.csv', index=False)
y_test_obj.to_csv('../data/processed/y_test.csv', index=False)

# í”¼ì²˜ ì´ë¦„ ì €ì¥
with open('../data/processed/feature_names_objective.txt', 'w') as f:
    f.write('\n'.join(X_train_obj.columns))
```

#### 3.3.3 ì£¼ìš” ê²°ê³¼ ìš”ì•½

**ì „ì²˜ë¦¬ í†µê³„**:
- **ì œê±°ëœ í”¼ì²˜**: 6ê°œ
- **ìƒì„±ëœ í”¼ì²˜**: 4ê°œ
- **ìµœì¢… í”¼ì²˜ ìˆ˜**:
  - Objective: 19ê°œ
  - Full: 21ê°œ
- **í•™ìŠµ ìƒ˜í”Œ**: 1,234ê°œ (80%)
- **í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ**: 309ê°œ (20%)

**âœ… ë‹¬ì„±ëœ ëª©í‘œ**:
- âœ… Data Leakage ë°©ì§€ (Objective vs Full ë¶„ë¦¬)
- âœ… í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€ (Stratified Split)
- âœ… Data Leakage ë°©ì§€ (Train fit, Test transform)
- âœ… ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ ìƒì„±
- âœ… ë¶ˆí•„ìš”í•œ í”¼ì²˜ ì œê±° (ê³¼ì í•© ë°©ì§€)

**ğŸ“Š ìƒì„±ëœ íŒŒì¼**:
- `data/processed/X_train_objective_scaled.csv`
- `data/processed/X_test_objective_scaled.csv`
- `data/processed/y_train.csv`
- `data/processed/y_test.csv`
- `models/scaler_objective.pkl`
- `models/scaler_full.pkl`

---

### Phase 4: ëª¨ë¸ë§ ë° í‰ê°€ (04_modeling_and_evaluation.ipynb)

#### 3.4.1 ëª©ì 
- ìµœì  ëª¨ë¸ ì„ íƒ
- Objective vs Full ì„±ëŠ¥ ë¹„êµ (Data Leakage ì •ëŸ‰í™”)
- í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
- ëª¨ë¸ í•œê³„ì  íŒŒì•…

#### 3.4.2 Baseline ëª¨ë¸

**Majority Class Predictor**:
```python
# í•­ìƒ ê°€ì¥ ë§ì€ í´ë˜ìŠ¤(MOS=4)ë¥¼ ì˜ˆì¸¡
from collections import Counter
most_common_class = Counter(y_train_obj).most_common(1)[0][0]
print(f"Most common class: {most_common_class}")  # Output: 4

# Baseline ì„±ëŠ¥
y_pred_baseline = [most_common_class] * len(y_test_obj)
baseline_accuracy = accuracy_score(y_test_obj, y_pred_baseline)
print(f"Baseline Accuracy: {baseline_accuracy:.1%}")
# Output: 50.8%
```

**Baseline í•´ì„**:
- ì•„ë¬´ê²ƒë„ ì•ˆ í•˜ê³  "ë¬´ì¡°ê±´ MOS=4"ë¼ê³  ë‹µí•˜ë©´ 50.8% ì •í™•ë„
- ìš°ë¦¬ ëª¨ë¸ì€ **ë°˜ë“œì‹œ** ì´ê²ƒë³´ë‹¤ ë‚˜ì•„ì•¼ ì˜ë¯¸ ìˆìŒ
- F1-Score: 0.342, Cohen's Kappa: 0.000

#### 3.4.3 ì•Œê³ ë¦¬ì¦˜ ì„ íƒ

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# 4ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ì´ìœ :

# 1. Logistic Regression: ë¹ ë¥´ê³  í•´ì„ ê°€ëŠ¥, Baseline++
models = {
    'Logistic Regression': LogisticRegression(
        max_iter=1000,
        class_weight='balanced',  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘
        random_state=42
    ),

    # 2. Decision Tree: ë¹„ì„ í˜• ê´€ê³„ í¬ì°©, ê³¼ì í•© ì§„ë‹¨ìš©
    'Decision Tree': DecisionTreeClassifier(
        max_depth=10,             # ê³¼ì í•© ì œí•œ
        min_samples_split=20,
        class_weight='balanced',
        random_state=42
    ),

    # 3. Random Forest: ê°•ë ¥í•œ ì•™ìƒë¸”, í”¼ì²˜ ì¤‘ìš”ë„
    'Random Forest': RandomForestClassifier(
        n_estimators=100,         # 100ê°œ íŠ¸ë¦¬
        max_depth=15,
        min_samples_split=20,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1                 # ë³‘ë ¬ ì²˜ë¦¬
    ),

    # 4. Gradient Boosting: ìµœê³  ì„±ëŠ¥ ê¸°ëŒ€
    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        min_samples_split=20,
        random_state=42
    )
}
```

#### 3.4.4 ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

**í‰ê°€ í•¨ìˆ˜ ì •ì˜**:
```python
from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, dataset_type):
    """ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""

    # í•™ìŠµ
    model.fit(X_train, y_train)

    # ì˜ˆì¸¡
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    train_acc = accuracy_score(y_train, y_pred_train)
    test_acc = accuracy_score(y_test, y_pred_test)
    f1_macro = f1_score(y_test, y_pred_test, average='macro')
    kappa = cohen_kappa_score(y_test, y_pred_test)
    overfit_gap = train_acc - test_acc

    # ê²°ê³¼ ì €ì¥
    results = {
        'Model': model_name,
        'Dataset': dataset_type,
        'Train_Accuracy': train_acc,
        'Test_Accuracy': test_acc,
        'F1_Score': f1_macro,
        'Cohen_Kappa': kappa,
        'Overfit_Gap': overfit_gap
    }

    # Confusion Matrix ì‹œê°í™”
    cm = confusion_matrix(y_test, y_pred_test)
    plt.figure(figsize=(8, 6), dpi=300)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])
    plt.title(f'{model_name} ({dataset_type}) - Confusion Matrix', fontsize=14)
    plt.xlabel('Predicted MOS', fontsize=12)
    plt.ylabel('Actual MOS', fontsize=12)
    plt.savefig(f'../results/figures/04_modeling_and_evaluation/{model_name}_{dataset_type}_cm.png',
                dpi=300, bbox_inches='tight')
    plt.close()

    return results, model
```

**ëª¨ë¸ í•™ìŠµ ë£¨í”„**:
```python
results_list = []

# Objective Dataset (ì‹¤ì œ ë°°í¬ ê°€ëŠ¥)
for name, model in models.items():
    print(f"\nTraining {name} (Objective)...")
    result, trained_model = evaluate_model(
        model, X_train_obj_scaled, y_train_obj,
        X_test_obj_scaled, y_test_obj,
        name, 'Objective'
    )
    results_list.append(result)

    # ëª¨ë¸ ì €ì¥
    joblib.dump(trained_model, f'../models/{name.replace(" ", "_")}_objective.pkl')

# Full Dataset (Data Leakage ì •ëŸ‰í™”)
for name, model in models.items():
    print(f"\nTraining {name} (Full)...")
    result, trained_model = evaluate_model(
        model, X_train_full_scaled, y_train_full,
        X_test_full_scaled, y_test_full,
        name, 'Full'
    )
    results_list.append(result)

    # ëª¨ë¸ ì €ì¥
    joblib.dump(trained_model, f'../models/{name.replace(" ", "_")}_full.pkl')
```

#### 3.4.5 ìµœì¢… ê²°ê³¼

**ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ**:

| ëª¨ë¸ | Dataset | Train Acc | Test Acc | F1 Score | Cohen Îº | Overfit Gap | í‰ê°€ |
|------|---------|-----------|----------|----------|---------|-------------|------|
| **Baseline** | - | 50.8% | 50.8% | 0.342 | 0.000 | 0.0% | ğŸ”´ ì´ê²¨ì•¼ í•¨ |
| **OBJECTIVE MODELS (ì‹¤ì œ ë°°í¬ ê°€ëŠ¥)** |
| Logistic Regression | Obj | 45.7% | 43.4% | 0.445 | 0.221 | 2.3% | ğŸ”´ Baselineë³´ë‹¤ ë‚®ìŒ |
| Decision Tree | Obj | 64.0% | 42.4% | 0.430 | 0.236 | 21.6% | ğŸ”´ ë‚®ìŒ + ê³¼ì í•© |
| Random Forest | Obj | 99.7% | 53.4% | 0.486 | 0.227 | 46.3% | ğŸŸ¡ ì•½ê°„ ê°œì„  |
| **Gradient Boosting** | **Obj** | **99.1%** | **59.5%** | **0.552** | **0.335** | **39.6%** | ğŸŸ¢ **ìµœê³ ** |
| **FULL MODELS (Data Leakage í¬í•¨)** |
| Logistic Regression | Full | 77.9% | 77.0% | 0.777 | 0.670 | 0.9% | ğŸ”µ ìš°ìˆ˜ (Leaky) |
| Decision Tree | Full | 98.9% | 75.4% | 0.754 | 0.642 | 23.5% | ğŸ”µ ìš°ìˆ˜ (Leaky) |
| **Random Forest** | **Full** | **99.0%** | **81.9%** | **0.819** | **0.727** | **17.2%** | ğŸ”µ **ìµœê³  (Leaky)** |
| Gradient Boosting | Full | 99.5% | 81.2% | 0.813 | 0.718 | 18.3% | ğŸ”µ ìš°ìˆ˜ (Leaky) |

**ê²°ê³¼ ì €ì¥**:
```python
results_df = pd.DataFrame(results_list)
results_df.to_csv('../results/metrics/model_comparison.csv', index=False)
print(results_df.to_string(index=False))
```

#### 3.4.6 í•µì‹¬ ë¶„ì„

**1) ìµœê³  ì‹¤ìš© ëª¨ë¸: Gradient Boosting (Objective)**

**ì„±ëŠ¥**:
- Test Accuracy: **59.5%** (Baseline 50.8% ëŒ€ë¹„ +8.7%p)
- F1-Score: **0.552** (ê· í˜•ì¡íŒ ì„±ëŠ¥)
- Cohen's Kappa: **0.335** ("fair agreement" - í†µê³„ì ìœ¼ë¡œ ìœ ì˜)

**í•´ì„**:
- âœ… Baselineì„ ëª…í™•íˆ ì´ê¹€ (í†µê³„ì ìœ¼ë¡œ ìœ ì˜)
- âš ï¸ í•˜ì§€ë§Œ 40.5% ì—ëŸ¬ìœ¨ (ì‹¤ìš©ì„± ì œí•œì )
- âš ï¸ 39.6% Overfit Gap (ì‹¬ê°í•œ ê³¼ì í•©)

**ì–¸ì œ ì‚¬ìš© ê°€ëŠ¥?**:
- âœ… íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§
- âœ… A/B í…ŒìŠ¤íŒ…
- âœ… ì¡°ê¸° ê²½ê³  ì‹œìŠ¤í…œ (ì‚¬ëŒ ê²€í† ìš©)
- âŒ ê³ ìœ„í—˜ ìë™í™” ê²°ì • (SLA í™˜ë¶ˆ, ë„¤íŠ¸ì›Œí¬ ì¬êµ¬ì„±)

**2) Data Leakage ì •ëŸ‰í™”**

**ì„±ëŠ¥ ì°¨ì´**:
```
Objective (ë°°í¬ ê°€ëŠ¥):  59.5% accuracy, Îº=0.335
Full (Leakage í¬í•¨):    81.9% accuracy, Îº=0.727

ì°¨ì´: +22.4%p accuracy, +0.392 Îº
```

**í•´ì„**:
- QoF_* í”¼ì²˜(ì£¼ê´€ì  í‰ê°€)ê°€ ì—„ì²­ë‚œ ì˜ˆì¸¡ë ¥ ì œê³µ
- í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì‚¬ìš© ë¶ˆê°€ (ì‚¬ìš©ìì—ê²Œ ì„¤ë¬¸ì¡°ì‚¬ í•„ìš”)
- EDAì—ì„œì˜ ê°€ì„¤(r=0.841) ì™„ë²½íˆ ê²€ì¦ë¨

**3) Confusion Matrix ë¶„ì„ (Gradient Boosting, Objective)**

```python
# Confusion Matrix (ì‹¤ì œ ê°’ vs ì˜ˆì¸¡ ê°’)
cm = confusion_matrix(y_test_obj, y_pred_gb_obj)
print(cm)
```

**ì¶œë ¥**:
```
ì‹¤ì œ\ì˜ˆì¸¡   1   2   3   4   5
1 (Bad)    [8  5  3  3  0]   â†’ Recall: 42%
2 (Poor)   [3  9  6  5  0]   â†’ Recall: 39%
3 (Fair)   [2  4 18 23  2]   â†’ Recall: 37%
4 (Good)   [1  2  8 132 14]  â†’ Recall: 84% âœ…
5 (Exc)    [0  1  3 42 15]   â†’ Recall: 25%
```

**í•µì‹¬ ë°œê²¬**:
- âœ… MOS=4 (Good) ì˜ˆì¸¡ ìš°ìˆ˜ (84% ì¬í˜„ìœ¨)
- âŒ MOS=5 (Excellent) ì˜ˆì¸¡ ì‹¤íŒ¨ (25%)
  - 80%ê°€ MOS=4ë¡œ ì˜¤ë¶„ë¥˜ë¨
  - ê°ê´€ì  ë©”íŠ¸ë¦­ìœ¼ë¡œëŠ” "Good"ì™€ "Excellent" êµ¬ë¶„ ì–´ë ¤ì›€
- âŒ MOS=1,2 (Bad/Poor) ì˜ˆì¸¡ ì–´ë ¤ì›€ (ìƒ˜í”Œ ë¶€ì¡±)

**4) í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„**

```python
# Gradient Boosting í”¼ì²˜ ì¤‘ìš”ë„
feature_importance = pd.DataFrame({
    'Feature': X_train_obj.columns,
    'Importance': gb_model_obj.feature_importances_
}).sort_values('Importance', ascending=False)

print(feature_importance.head(10))
```

**ê²°ê³¼ (Top 10)**:
```
Rank | Feature                  | Importance | í•´ì„
-----|--------------------------|------------|------------------------
1    | QoA_BUFFERINGtime        | ~14.2%     | ë²„í¼ë§ ì‹œê°„ (ì›ë³¸)
2    | QoA_BUFFERINGtime_log    | ~13.1%     | ë²„í¼ë§ ì‹œê°„ (ë¡œê·¸)
3    | Video_Quality_Index      | ~10.4%     | ì¢…í•© ë¹„ë””ì˜¤ í’ˆì§ˆ â­
4    | QoA_VLCframerate         | ~9.5%      | í”„ë ˆì„ë ˆì´íŠ¸
5    | Buffering_Severity       | ~9.4%      | ë²„í¼ë§ ì‹¬ê°ë„ â­
6    | QoA_VLCbitrate           | ~7.7%      | ë¹„íŠ¸ë ˆì´íŠ¸
7    | QoU_age                  | ~6.6%      | ì—°ë ¹ (ë†€ëê²Œë„ ë†’ìŒ)
8    | Audio_Quality_Adjusted   | ~6.6%      | ì˜¤ë””ì˜¤ í’ˆì§ˆ
9    | QoA_VLCaudiorate         | ~5.6%      | ì˜¤ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸
10   | QoD_api-level            | ~2.8%      | ì•ˆë“œë¡œì´ë“œ API ë ˆë²¨
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- **ë²„í¼ë§ ì§€ë°°**: 1ìœ„+2ìœ„ = 27.3%, 1ìœ„+2ìœ„+5ìœ„ = 36.6%
- **Feature Engineering ì„±ê³µ**: â­ í‘œì‹œëœ í”¼ì²˜ê°€ ëª¨ë‘ ìƒìœ„ 10ìœ„
- **ë†€ë¼ìš´ ë°œê²¬**: ì—°ë ¹ì´ ë„¤íŠ¸ì›Œí¬ íƒ€ì…ë³´ë‹¤ ì¤‘ìš” (ì˜ˆìƒ ë°–)

**ì‹œê°í™”**:
```python
plt.figure(figsize=(10, 8), dpi=300)
feature_importance.head(15).plot(x='Feature', y='Importance', kind='barh',
                                  color='steelblue', legend=False)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title('Top 15 Feature Importance (Gradient Boosting, Objective)', fontsize=14)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('../results/figures/04_modeling_and_evaluation/6_feature_importance_gb.png',
            dpi=300, bbox_inches='tight')
```

**5) ëª¨ë¸ ë¹„êµ ì‹œê°í™”**

```python
# ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=300)

# Test Accuracy
results_df.pivot(index='Model', columns='Dataset', values='Test_Accuracy').plot(
    kind='bar', ax=axes[0,0], color=['steelblue', 'coral'], rot=45)
axes[0,0].set_title('Test Accuracy by Model', fontsize=14)
axes[0,0].axhline(y=0.508, color='red', linestyle='--', label='Baseline')
axes[0,0].legend()

# F1 Score
results_df.pivot(index='Model', columns='Dataset', values='F1_Score').plot(
    kind='bar', ax=axes[0,1], color=['steelblue', 'coral'], rot=45)
axes[0,1].set_title('F1 Score by Model', fontsize=14)

# Cohen's Kappa
results_df.pivot(index='Model', columns='Dataset', values='Cohen_Kappa').plot(
    kind='bar', ax=axes[1,0], color=['steelblue', 'coral'], rot=45)
axes[1,0].set_title("Cohen's Kappa by Model", fontsize=14)

# Overfit Gap
results_df.pivot(index='Model', columns='Dataset', values='Overfit_Gap').plot(
    kind='bar', ax=axes[1,1], color=['steelblue', 'coral'], rot=45)
axes[1,1].set_title('Overfit Gap by Model', fontsize=14)
axes[1,1].axhline(y=0.15, color='orange', linestyle='--', label='Warning (15%)')
axes[1,1].legend()

plt.tight_layout()
plt.savefig('../results/figures/04_modeling_and_evaluation/8_model_comparison.png',
            dpi=300, bbox_inches='tight')
```

#### 3.4.7 ì£¼ìš” ê²°ê³¼ ìš”ì•½

**âœ… ì„±ê³µ**:
- Baseline(50.8%) ëŒ€ë¹„ +8.7%p ê°œì„ 
- Data Leakage ì •ëŸ‰í™” (22.4%p ì°¨ì´)
- Feature Engineering ìœ íš¨ì„± ì¦ëª…
- ë²„í¼ë§ì´ ìµœëŒ€ ì˜í–¥ ìš”ì¸ í™•ì¸ (36.6%)

**âš ï¸ í•œê³„ì **:
1. **ì„±ëŠ¥ ì œí•œì **: 59.5% (40.5% ì—ëŸ¬ìœ¨)
2. **ì‹¬ê°í•œ ê³¼ì í•©**: 39.6% Train-Test gap
3. **MOS=5 ì˜ˆì¸¡ ì‹¤íŒ¨**: ì¬í˜„ìœ¨ 25%ë§Œ
4. **í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¯¸í•´ê²°**: ì—¬ì „íˆ MOS=4 í¸í–¥
5. **ë°ì´í„° ë¶€ì¡±**: 1,234 í•™ìŠµ ìƒ˜í”Œ (ë” ë§ì´ í•„ìš”)

**ğŸ“Š ìƒì„±ëœ íŒŒì¼** (6ê°œ + 1ê°œ):
- `4_confusion_matrix_logistic_regression.png` (DPI 300)
- `5_confusion_matrix_decision_tree.png` (DPI 300)
- `6_confusion_matrix_random_forest.png` (DPI 300)
- `6_feature_importance_rf.png` (DPI 300)
- `7_confusion_matrix_gradient_boosting.png` (DPI 300)
- `8_model_comparison.png` (DPI 300)
- `results/metrics/model_comparison.csv`

---

## 4. ì£¼ìš” ë°œê²¬ ë° ì¸ì‚¬ì´íŠ¸

### 4.1 ì„±ê³µì ì¸ ë°œê²¬

#### 4.1.1 ë²„í¼ë§ì´ ì™•
**ì¦ê±°**:
- ìƒê´€ê³„ìˆ˜: r = -0.482 (ê°•í•œ ìŒì˜ ìƒê´€)
- í”¼ì²˜ ì¤‘ìš”ë„: 36.6% (1ìœ„+2ìœ„+5ìœ„ í•©ì‚°)
- ì‚¬ìš©ì í—ˆìš© ì„ê³„ê°’: 2íšŒê¹Œì§€ ìˆ˜ìš©, 3íšŒë¶€í„° ë¶ˆë§Œì¡±

**ì‹¤ë¬´ ì˜ë¯¸**:
- ë²„í¼ë§ ìµœì†Œí™” = ê°€ì¥ ì¤‘ìš”í•œ ê¸°ìˆ ì  ë ˆë²„
- ë„¤íŠ¸ì›Œí¬ íˆ¬ì ìš°ì„ ìˆœìœ„: CDN, ìºì‹±, ì ì‘í˜• ë¹„íŠ¸ë ˆì´íŠ¸

#### 4.1.2 Data Leakage ì •ëŸ‰í™”
**ì¦ê±°**:
- Objective: 59.5% accuracy, Îº=0.335
- Full: 81.9% accuracy, Îº=0.727
- ì°¨ì´: **22.4%p**

**í•™ìˆ ì  ì˜ë¯¸**:
- ë°©ë²•ë¡ ì  ì—„ê²©í•¨ì˜ ì¤‘ìš”ì„± ì¦ëª…
- "ì¢‹ì€ ê²°ê³¼"ë¥¼ ë³´ê³ í•˜ëŠ” ê²ƒë³´ë‹¤ "ì˜¬ë°”ë¥¸ ê²°ê³¼"ë¥¼ ë³´ê³ í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”
- Data Leakage ê²½ê³„ ì‚¬ë¡€ë¡œ êµìœ¡ ìë£Œ í™œìš© ê°€ëŠ¥

#### 4.1.3 Feature Engineering ì„±ê³µ
**ì¦ê±°**:
- Buffering_Severity: 5ìœ„ (9.4%)
- Video_Quality_Index: 3ìœ„ (10.4%)
- Network_Generation: ìƒìœ„ 15ìœ„ ì§„ì…

**ë°©ë²•ë¡ ì  ì˜ë¯¸**:
- ë„ë©”ì¸ ì§€ì‹ + ì°½ì˜ì„± = ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ
- ë‹¨ìˆœ ì›ë³¸ í”¼ì²˜ë³´ë‹¤ ë³µí•© í”¼ì²˜ê°€ ë” ê°•ë ¥

#### 4.1.4 ë„¤íŠ¸ì›Œí¬ ì„¸ëŒ€ë³„ ëª…í™•í•œ ì°¨ì´
**ì¦ê±°**:
- 2G (EDGE): í‰ê·  MOS 1.56 (ê±°ì˜ ì‹œì²­ ë¶ˆê°€)
- 3G (UMTS/HSPA): í‰ê·  MOS 3.4~3.8 (ìˆ˜ìš© ê°€ëŠ¥)
- 4G (LTE): í‰ê·  MOS 3.78 (ìš°ìˆ˜)

**ì •ì±… ì˜ë¯¸**:
- 2G ì„œë¹„ìŠ¤ ì¢…ë£Œ ì •ë‹¹í™”
- 3G ìµœì†Œ ì¸í”„ë¼ ìœ ì§€ í•„ìš”
- 4G/5G í™•ëŒ€ íˆ¬ì íƒ€ë‹¹ì„±

### 4.2 í•œê³„ì  (Critical Assessment)

#### 4.2.1 ì„±ëŠ¥ ì œí•œì 
**ì‚¬ì‹¤**:
- Gradient Boosting: 59.5% accuracy
- Cohen's Kappa: 0.335 ("fair agreement")
- ì—ëŸ¬ìœ¨: 40.5%

**ì˜ë¯¸**:
- í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ë§Œ ì‹¤ìš©ì„± ì œí•œì 
- ê³ ìœ„í—˜ ìë™í™” ì‘ì—…ì—ëŠ” ë¶€ì í•©
- "ì‚¬ëŒ ë³´ì¡°" ìˆ˜ì¤€ì— ë¨¸ë¬¼ëŸ¬ì•¼ í•¨

**ì›ì¸ ë¶„ì„**:
1. ë°ì´í„° ë¶€ì¡± (1,234 í•™ìŠµ ìƒ˜í”Œ)
2. í”¼ì²˜ ì œí•œ (ì£¼ê´€ì  ìš”ì†Œ ì œì™¸)
3. í´ë˜ìŠ¤ ë¶ˆê· í˜• (MOS=4ê°€ 50.8%)
4. ê°œì¸ ì°¨ì´ (ê°™ì€ ì¡°ê±´ì—ì„œë„ MOS ë‹¤ë¦„)

#### 4.2.2 ì‹¬ê°í•œ ê³¼ì í•©
**ì‚¬ì‹¤**:
- Gradient Boosting: 39.6% Overfit Gap
- Random Forest: 46.3% Overfit Gap
- Train: 99%+, Test: 53~59%

**ì˜ë¯¸**:
- ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë§Œ ì•”ê¸°
- ìƒˆë¡œìš´ ë°ì´í„°ì— ì¼ë°˜í™” ì‹¤íŒ¨
- ì‹¤ì œ ë°°í¬ ì‹œ ì„±ëŠ¥ ë” ë‚®ì„ ê°€ëŠ¥ì„±

**ì›ì¸**:
1. ë°ì´í„° ë¶€ì¡± (ìƒ˜í”Œ ìˆ˜ < í”¼ì²˜ ìˆ˜ Ã— 100)
2. íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì˜ ê³¼ì í•© ê²½í–¥
3. Hyperparameter tuning ë¶€ì¡±

**í•´ê²°ì±…**:
- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ (10,000+ ìƒ˜í”Œ)
- GridSearchCVë¡œ ì •ê·œí™” ê°•í™”
- Cross-validationìœ¼ë¡œ robust í‰ê°€

#### 4.2.3 "Excellent" ì˜ˆì¸¡ ì‹¤íŒ¨
**ì‚¬ì‹¤**:
- MOS=5 ì¬í˜„ìœ¨: 25%
- 75%ê°€ MOS=4ë¡œ ì˜¤ë¶„ë¥˜

**ì˜ë¯¸**:
- ê°ê´€ì  ë©”íŠ¸ë¦­ìœ¼ë¡œëŠ” "Good"ì™€ "Excellent" êµ¬ë¶„ ë¶ˆê°€
- ì£¼ê´€ì  ìš”ì†Œ (ê¸°ë¶„, ê¸°ëŒ€, ì»¨í…ìŠ¤íŠ¸)ê°€ ì¤‘ìš”

**ì´ë¡ ì  í•¨ì˜**:
- QoEì˜ ì£¼ê´€ì  ë³¸ì§ˆ
- ITU-T P.10 ê¶Œê³ ì•ˆ: "QoEëŠ” ê¸°ìˆ ì  ìš”ì†Œ ì™¸ì— ì‹¬ë¦¬ì /ì‚¬íšŒì  ìš”ì†Œ í¬í•¨"
- ì™„ë²½í•œ ê°ê´€ì  ì˜ˆì¸¡ì€ ë¶ˆê°€ëŠ¥í•  ìˆ˜ë„ ìˆìŒ

#### 4.2.4 í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¯¸í•´ê²°
**ì‚¬ì‹¤**:
- `class_weight='balanced'` ì‚¬ìš©í–ˆì§€ë§Œ ì—¬ì „íˆ í¸í–¥
- MOS=4 ì¬í˜„ìœ¨: 84% vs MOS=1,2 ì¬í˜„ìœ¨: 40%

**ì˜ë¯¸**:
- ì†Œìˆ˜ í´ë˜ìŠ¤ (Bad/Poor) ì˜ˆì¸¡ ì–´ë ¤ì›€
- ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ì¼€ì´ìŠ¤! (ë¶ˆë§Œì¡± ì‚¬ìš©ì ì¡°ê¸° ë°œê²¬)

**í•´ê²°ì±…**:
- **SMOTE** (Synthetic Minority Over-sampling Technique)
  - ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ìƒ˜í”Œ ìƒì„±
  - 5~10%p ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ
- **Cost-sensitive Learning**
  - MOS=1,2 ì˜¤ë¶„ë¥˜ í˜ë„í‹° ì¦ê°€
- **Threshold Optimization**
  - í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì„ê³„ê°’ ìµœì í™”

#### 4.2.5 ì¼ë°˜í™” ìš°ë ¤
**ë°ì´í„° í•œê³„**:
1. **2015ë…„ ë°ì´í„°**:
   - 5G, VP9/H.265 ì½”ë± ë¯¸ë°˜ì˜
   - í˜„ì¬ ì‚¬ìš©ì ê¸°ëŒ€ì¹˜ì™€ ë‹¤ë¦„
2. **ì§€ì—­ íŠ¹í™”**:
   - í”„ë‘ìŠ¤ 4ê°œ í†µì‹ ì‚¬ë§Œ
   - ë‹¤ë¥¸ êµ­ê°€/ë¬¸í™”ì—ì„œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
3. **ì¸êµ¬ í¸í–¥**:
   - 19-38ì„¸ ì—°êµ¬ì/í•™ìƒë§Œ
   - ì–´ë¦°ì´/ë…¸ì¸ í¬í•¨ ì•ˆ ë¨
4. **ë‹¨ì¼ ë¹„ë””ì˜¤**:
   - Big Buck Bunnyë§Œ
   - ì½˜í…ì¸  íƒ€ì…(ìŠ¤í¬ì¸ /ì˜í™”) ì˜í–¥ ë¯¸ë°˜ì˜

**ì˜ë¯¸**:
- ì´ ëª¨ë¸ì„ 2024ë…„ ê¸€ë¡œë²Œ ë°°í¬ëŠ” ìœ„í—˜
- ë„ë©”ì¸ ì „ì´ (Domain Adaptation) í•„ìš”
- ì§€ì†ì  ë°ì´í„° ìˆ˜ì§‘ ë° ì¬í•™ìŠµ í•„ìˆ˜

### 4.3 ì‹¤ì œ í™œìš© ê°€ëŠ¥ì„±

#### 4.3.1 ê°€ëŠ¥í•œ ê²ƒ âœ…

**1) íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§**
```python
# ì£¼ê°„ í‰ê·  ì˜ˆì¸¡ MOS
weekly_mos = model.predict(new_data).mean()
# 3.8 â†’ 3.6 â†’ 3.4: í’ˆì§ˆ ì €í•˜ íŠ¸ë Œë“œ ê°ì§€
```

**2) A/B í…ŒìŠ¤íŒ…**
```python
# CDN ë³€ê²½ ì „í›„ ë¹„êµ
before_mos = 3.5 (Â± 0.1)
after_mos = 3.7 (Â± 0.1)
# â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê°œì„  (p<0.05)
```

**3) ì¡°ê¸° ê²½ê³  ì‹œìŠ¤í…œ**
```python
# ì˜ˆì¸¡ MOS < 3.0ì¸ ì„¸ì…˜ í”Œë˜ê·¸
if predicted_mos < 3.0:
    alert_engineer()  # ì‚¬ëŒì´ í™•ì¸
```

**4) ìƒëŒ€ì  ë¹„êµ**
```python
# ë””ë°”ì´ìŠ¤ A vs B
device_a_mos = 3.6
device_b_mos = 3.9
# â†’ Device Bê°€ ë” ë‚˜ìŒ (ì ˆëŒ€ê°’ì€ ë¶€ì •í™•í•´ë„ ìˆœìœ„ëŠ” ë§ìŒ)
```

#### 4.3.2 ë¶ˆê°€ëŠ¥í•œ ê²ƒ âŒ

**1) SLA ìë™ í™˜ë¶ˆ**
```python
# ìœ„í—˜: 40.5% ì—ëŸ¬ìœ¨
if predicted_mos < 3.0:
    refund_customer()  # â† 40%ê°€ ì˜¤íŒ!
```

**2) ë„¤íŠ¸ì›Œí¬ ìë™ ì¬êµ¬ì„±**
```python
# ìœ„í—˜: ê³¼ì í•©ìœ¼ë¡œ ìƒˆë¡œìš´ ì¡°ê±´ì—ì„œ ì‹¤íŒ¨
if predicted_mos < 3.5:
    reconfigure_network()  # â† í° ì˜í–¥, ë‚®ì€ ì‹ ë¢°ë„
```

**3) ê°œì¸ë³„ QoE ë³´ì¥**
```python
# ë¶ˆê°€ëŠ¥: MOS=5 ì˜ˆì¸¡ ì¬í˜„ìœ¨ 25%
guarantee_excellent_experience(user_id)  # â† ì‹¤íŒ¨
```

**4) ë²•ì  ë¶„ìŸ ì¦ê±°**
```python
# ë¶€ì í•©: Cohen's Kappa 0.335 ("fair")
use_in_court(predicted_mos)  # â† ì‹ ë¢°ë„ ë¶ˆì¶©ë¶„
```

---

## 5. í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡°

```
Poqemon-QoE-Dataset-master/
â”‚
â”œâ”€â”€ data/                                 # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ raw/                              # ì›ë³¸ ë°ì´í„° (ì½ê¸° ì „ìš©)
â”‚   â”‚   â”œâ”€â”€ pokemon.csv                   # 1,543 samples Ã— 23 features (ë©”ì¸)
â”‚   â”‚   â”œâ”€â”€ pokemon.arff                  # WEKA í˜•ì‹
â”‚   â”‚   â”œâ”€â”€ pokemon.data                  # UCI ML Repository í˜•ì‹
â”‚   â”‚   â””â”€â”€ pokemon.names                 # ë©”íƒ€ë°ì´í„° ì„¤ëª…
â”‚   â”‚
â”‚   â””â”€â”€ processed/                        # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚       â”œâ”€â”€ X_train_objective_scaled.csv  # í•™ìŠµ í”¼ì²˜ (Objective, 1234Ã—19)
â”‚       â”œâ”€â”€ X_test_objective_scaled.csv   # í…ŒìŠ¤íŠ¸ í”¼ì²˜ (Objective, 309Ã—19)
â”‚       â”œâ”€â”€ X_train_full_scaled.csv       # í•™ìŠµ í”¼ì²˜ (Full, 1234Ã—21)
â”‚       â”œâ”€â”€ X_test_full_scaled.csv        # í…ŒìŠ¤íŠ¸ í”¼ì²˜ (Full, 309Ã—21)
â”‚       â”œâ”€â”€ y_train.csv                   # í•™ìŠµ íƒ€ê²Ÿ (1234Ã—1)
â”‚       â”œâ”€â”€ y_test.csv                    # í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ (309Ã—1)
â”‚       â”œâ”€â”€ feature_names_objective.txt   # Objective í”¼ì²˜ ëª©ë¡ (19ê°œ)
â”‚       â””â”€â”€ feature_names_full.txt        # Full í”¼ì²˜ ëª©ë¡ (21ê°œ)
â”‚
â”œâ”€â”€ notebooks/                            # Jupyter ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ 01_data_understanding.ipynb       # Phase 1: ë°ì´í„° ì´í•´
â”‚   â”œâ”€â”€ 02_exploratory_data_analysis.ipynb # Phase 2: EDA
â”‚   â”œâ”€â”€ 03_data_preprocessing.ipynb       # Phase 3: ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ 04_modeling_and_evaluation.ipynb  # Phase 4: ëª¨ë¸ë§
â”‚   â”‚
â”‚   â”œâ”€â”€ ANALYSIS_01_DATA_UNDERSTANDING.md # ë¶„ì„ ë³´ê³ ì„œ (ë§ˆí¬ë‹¤ìš´)
â”‚   â”œâ”€â”€ ANALYSIS_02_EXPLORATORY_DATA_ANALYSIS.md
â”‚   â”œâ”€â”€ ANALYSIS_03_DATA_PREPROCESSING.md
â”‚   â””â”€â”€ ANALYSIS_04_MODELING_EVALUATION.md
â”‚
â”œâ”€â”€ results/                              # ê²°ê³¼ë¬¼
â”‚   â”œâ”€â”€ figures/                          # ì‹œê°í™” (DPI 300+)
â”‚   â”‚   â”œâ”€â”€ 01_data_understanding/        # Phase 1 ê·¸ë˜í”„ (1ê°œ)
â”‚   â”‚   â”‚   â””â”€â”€ mos_distribution.png
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ 02_exploratory_data_analysis/ # Phase 2 ê·¸ë˜í”„ (5ê°œ)
â”‚   â”‚   â”‚   â”œâ”€â”€ 2_correlation_matrix.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 3_network_type_vs_mos.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 4_buffering_vs_mos.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 5_resolution_vs_mos.png
â”‚   â”‚   â”‚   â””â”€â”€ 6_demographics_vs_mos.png
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ 04_modeling_and_evaluation/   # Phase 4 ê·¸ë˜í”„ (6ê°œ)
â”‚   â”‚   â”‚   â”œâ”€â”€ 4_confusion_matrix_logistic_regression.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 5_confusion_matrix_decision_tree.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 6_confusion_matrix_random_forest.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 6_feature_importance_rf.png
â”‚   â”‚   â”‚   â”œâ”€â”€ 7_confusion_matrix_gradient_boosting.png
â”‚   â”‚   â”‚   â””â”€â”€ 8_model_comparison.png
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ archive_old_files/            # êµ¬ë²„ì „ ë°±ì—…
â”‚   â”‚
â”‚   â””â”€â”€ metrics/                          # ì •ëŸ‰ì  ê²°ê³¼
â”‚       â””â”€â”€ model_comparison.csv          # ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ
â”‚
â”œâ”€â”€ models/                               # ì €ì¥ëœ ëª¨ë¸
â”‚   â”œâ”€â”€ scaler_objective.pkl              # StandardScaler (Objective)
â”‚   â”œâ”€â”€ scaler_full.pkl                   # StandardScaler (Full)
â”‚   â”œâ”€â”€ Logistic_Regression_objective.pkl # í•™ìŠµëœ ëª¨ë¸ë“¤...
â”‚   â”œâ”€â”€ Decision_Tree_objective.pkl
â”‚   â”œâ”€â”€ Random_Forest_objective.pkl
â”‚   â”œâ”€â”€ Gradient_Boosting_objective.pkl
â”‚   â”œâ”€â”€ Logistic_Regression_full.pkl
â”‚   â”œâ”€â”€ Decision_Tree_full.pkl
â”‚   â”œâ”€â”€ Random_Forest_full.pkl
â”‚   â””â”€â”€ Gradient_Boosting_full.pkl
â”‚
â”œâ”€â”€ src/                                  # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ data_loader.py                # ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_utils.py                # ëª¨ë¸ í‰ê°€/ì €ì¥ í•¨ìˆ˜
â”‚   â”‚
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ plot_utils.py                 # ì‹œê°í™” ìœ í‹¸ë¦¬í‹°
â”‚
â”œâ”€â”€ docs/                                 # ë¬¸ì„œ
â”‚   â”œâ”€â”€ PROJECT_GUIDE.md                  # í”„ë¡œì íŠ¸ ì „ì²´ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ DATASET_DESCRIPTION.md            # ë°ì´í„°ì…‹ ìƒì„¸ ì„¤ëª…
â”‚   â”œâ”€â”€ GUIDELINES_KR.md                  # Alessandro Maddaloni ê°€ì´ë“œë¼ì¸
â”‚   â”œâ”€â”€ COMPLETION_SUMMARY.md             # í”„ë¡œì íŠ¸ ì™„ë£Œ ìš”ì•½
â”‚   â””â”€â”€ REFERENCES.md                     # ì°¸ê³  ë¬¸í—Œ
â”‚
â”œâ”€â”€ reports/                              # ìµœì¢… ë³´ê³ ì„œ
â”‚   â””â”€â”€ Pokemon QoE Dataset Final Analysis Report_Changyong Hyun.pdf
â”‚       # 28í˜ì´ì§€ ìµœì¢… ë³´ê³ ì„œ (Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜)
â”‚
â”œâ”€â”€ skywork/                              # AI ë³´ê³ ì„œ ìƒì„±ìš© (ì •ë¦¬ëœ íŒŒì¼)
â”‚   â”œâ”€â”€ 01_analysis_reports/              # 4ê°œ ë¶„ì„ ë§ˆí¬ë‹¤ìš´
â”‚   â”‚   â”œâ”€â”€ ANALYSIS_01_DATA_UNDERSTANDING.md
â”‚   â”‚   â”œâ”€â”€ ANALYSIS_02_EXPLORATORY_DATA_ANALYSIS.md
â”‚   â”‚   â”œâ”€â”€ ANALYSIS_03_DATA_PREPROCESSING.md
â”‚   â”‚   â””â”€â”€ ANALYSIS_04_MODELING_EVALUATION.md
â”‚   â”‚
â”‚   â”œâ”€â”€ 02_project_docs/                  # 3ê°œ í”„ë¡œì íŠ¸ ë¬¸ì„œ
â”‚   â”‚   â”œâ”€â”€ PROJECT_GUIDE.md
â”‚   â”‚   â”œâ”€â”€ DATASET_DESCRIPTION.md
â”‚   â”‚   â””â”€â”€ GUIDELINES_KR.md
â”‚   â”‚
â”‚   â”œâ”€â”€ 03_results/                       # ê²°ê³¼ CSV
â”‚   â”‚   â””â”€â”€ model_comparison.csv
â”‚   â”‚
â”‚   â””â”€â”€ 04_visualizations/                # í•µì‹¬ ì‹œê°í™” (8ê°œ)
â”‚       â”œâ”€â”€ mos_distribution.png
â”‚       â”œâ”€â”€ correlation_matrix.png
â”‚       â”œâ”€â”€ network_type_vs_mos.png
â”‚       â”œâ”€â”€ buffering_vs_mos.png
â”‚       â”œâ”€â”€ confusion_matrix_gradient_boosting.png
â”‚       â”œâ”€â”€ feature_importance_rf.png
â”‚       â”œâ”€â”€ model_comparison.png
â”‚       â””â”€â”€ [ê¸°íƒ€ í•µì‹¬ ê·¸ë˜í”„]
â”‚
â”œâ”€â”€ README.md                             # í”„ë¡œì íŠ¸ ì†Œê°œ
â”œâ”€â”€ requirements.txt                      # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â””â”€â”€ .gitignore                            # Git ì œì™¸ íŒŒì¼

```

### íŒŒì¼ ì„¤ëª…

#### ë°ì´í„° íŒŒì¼
- **pokemon.csv**: ë©”ì¸ ë°ì´í„°ì…‹ (CSV í˜•ì‹, ê°€ì¥ ë„ë¦¬ ì‚¬ìš©)
- **pokemon.arff**: WEKA ML íˆ´ìš© í˜•ì‹
- **pokemon.data**: UCI ML Repository í‘œì¤€ í˜•ì‹
- **pokemon.names**: í”¼ì²˜ ì„¤ëª… ë° ë©”íƒ€ë°ì´í„°

#### ë…¸íŠ¸ë¶
- **01~04.ipynb**: 4ë‹¨ê³„ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ (ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ)
- **ANALYSIS_*.md**: ë…¸íŠ¸ë¶ì˜ ë§ˆí¬ë‹¤ìš´ ë²„ì „ (GitHubì—ì„œ ì½ê¸° í¸í•¨)

#### ê²°ê³¼ íŒŒì¼
- **figures/**: ëª¨ë“  ì‹œê°í™” (12ê°œ PNG, DPI 300+)
- **model_comparison.csv**: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ (ì—‘ì…€ë¡œ ì—´ê¸° ê°€ëŠ¥)

#### ëª¨ë¸ íŒŒì¼
- **scaler_*.pkl**: ì‹¤ì œ ë°°í¬ ì‹œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ëŸ¬
- **ëª¨ë¸_*.pkl**: í•™ìŠµëœ ëª¨ë¸ (ì¬ì‚¬ìš© ê°€ëŠ¥)

#### ë¬¸ì„œ
- **PROJECT_GUIDE.md**: ì „ì²´ í”„ë¡œì íŠ¸ ê°€ì´ë“œ (ì´ íŒŒì¼ê³¼ ìœ ì‚¬)
- **GUIDELINES_KR.md**: Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ í•œêµ­ì–´ ë²ˆì—­

---

## 6. Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜

### 6.1 ê°€ì´ë“œë¼ì¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **Telecom SudParis**ì˜ **Alessandro Maddaloni êµìˆ˜ë‹˜**ì´ ì œì‹œí•œ ì—„ê²©í•œ í•™ìˆ  ê¸°ì¤€ì„ ë”°ë¦…ë‹ˆë‹¤.

**ì¶œì²˜**:
- ê³¼ëª©: Data Science - Theory to Practice
- ê¸°ê´€: Telecom SudParis, Institut Polytechnique de Paris
- í•™ë…„: 2024-2025

### 6.2 í•µì‹¬ ì›ì¹™

#### 6.2.1 WHY before WHAT

**ì›ì¹™**: ëª¨ë“  ë¶„ì„/ì•Œê³ ë¦¬ì¦˜ ì ìš© ì „ì— "ì™œ ì´ê²ƒì„ í•˜ëŠ”ê°€?"ë¥¼ ì„¤ëª…

**ë³¸ í”„ë¡œì íŠ¸ ì ìš© ì˜ˆì‹œ**:

```markdown
# ë‚˜ìœ ì˜ˆ (WHATë§Œ ì œì‹œ)
"We used Gradient Boosting."

# ì¢‹ì€ ì˜ˆ (WHY ë¨¼ì €, WHAT ë‚˜ì¤‘)
"WHY: Given the non-linear relationships discovered in EDA (e.g.,
buffering's exponential impact on MOS), we needed an algorithm capable
of capturing complex interactions. Additionally, the class imbalance
required a robust approach.

WHAT: We selected Gradient Boosting because it:
1. Handles non-linearity through sequential tree building
2. Supports class weights for imbalance
3. Provides feature importance for interpretability"
```

**í”„ë¡œì íŠ¸ ì „ì²´ì— ì ìš©**:
- Phase 1: "WHY: Before any analysis, we must understand..."
- Phase 2: "WHY: The initial findings suggested potential data leakage..."
- Phase 3: "WHY: To prevent data leakage, we must..."
- Phase 4: "WHY: To quantify data leakage, we created two separate..."

#### 6.2.2 Expected vs Actual

**ì›ì¹™**: ê°€ì„¤ ìˆ˜ë¦½ â†’ ê²€ì¦ â†’ ë¹„êµ â†’ í•´ì„

**ë³¸ í”„ë¡œì íŠ¸ ì ìš© ì˜ˆì‹œ**:

```markdown
# Phase 2 EDAì—ì„œ 4ê°œ ê°€ì„¤ ì²´ê³„ì  ê²€ì¦

ê°€ì„¤ 1: ë²„í¼ë§ì´ MOSì— ê°€ì¥ í° ì˜í–¥
- Expected: r < -0.4 (ê°•í•œ ìŒì˜ ìƒê´€)
- Actual: r = -0.482
- Conclusion: âœ… ê°€ì„¤ í™•ì¸, ì˜ˆìƒë³´ë‹¤ ì•½ê°„ ë” ê°•í•¨

ê°€ì„¤ 4: QoF_* í”¼ì²˜ê°€ Data Leakage
- Expected: r > 0.7 (ë§¤ìš° ê°•í•œ ìƒê´€)
- Actual: r = 0.841
- Conclusion: âœ… ê°€ì„¤ í™•ì¸, Data Leakage í™•ì •
```

#### 6.2.3 Critical Assessment

**ì›ì¹™**: í•œê³„ì ì„ ì†”ì§íˆ ì¸ì •, "íŒë§¤" ì‹œë„ ê¸ˆì§€

**ë³¸ í”„ë¡œì íŠ¸ ì ìš©**:

```markdown
# ë‚˜ìœ ì˜ˆ (ê³¼ì¥)
"Our model achieves 59.5% accuracy, demonstrating excellent performance
for QoE prediction."

# ì¢‹ì€ ì˜ˆ (ì†”ì§í•¨)
"Our model achieves 59.5% accuracy, which is statistically significant
above baseline (50.8%) but still results in a 40.5% error rate. This
limits practical applicability to:
âœ… Trend monitoring
âœ… A/B testing
âŒ High-stakes automated decisions (SLA refunds, network reconfiguration)

The model suffers from:
- Severe overfitting (39.6% train-test gap)
- Poor MOS=5 recall (25%)
- Generalization concerns (2015 data, regional bias)"
```

**5ê°€ì§€ í•œê³„ì  ëª…ì‹œ** (ì„¹ì…˜ 4.2 ì°¸ì¡°):
1. ì„±ëŠ¥ ì œí•œì  (59.5%)
2. ì‹¬ê°í•œ ê³¼ì í•© (39.6% gap)
3. "Excellent" ì˜ˆì¸¡ ì‹¤íŒ¨ (25% ì¬í˜„ìœ¨)
4. í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¯¸í•´ê²°
5. ì¼ë°˜í™” ìš°ë ¤ (ë°ì´í„° í•œê³„)

#### 6.2.4 Selective Reporting

**ì›ì¹™**: ëª¨ë“  ê²ƒì„ ë³´ê³ í•˜ì§€ ë§ê³ , ì˜ë¯¸ìˆëŠ” ê²ƒë§Œ ë³´ê³ 

**ë³¸ í”„ë¡œì íŠ¸ ì ìš©**:

**âŒ í”¼í•œ ê²ƒ**:
- Raw Python output ê¸´ ë¡œê·¸
- ì¤‘ë³µëœ ì‹œê°í™” (ê°™ì€ ì •ë³´ë¥¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ)
- í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•Šì€ ê´€ê³„
- ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•© ê²°ê³¼

**âœ… í¬í•¨í•œ ê²ƒ**:
- í•µì‹¬ ë°œê²¬ (ë²„í¼ë§ ì˜í–¥, Data Leakage)
- í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼ (ANOVA p<0.001)
- ì‹¤ë¬´ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ (í—ˆìš© ì„ê³„ê°’)
- í•œê³„ì  (Critical Assessment)

**ì‹œê°í™” ì„ íƒ**:
```
EDA ì¤‘ ìƒì„±í•œ ê·¸ë˜í”„: ~20ê°œ
ìµœì¢… ë³´ê³ ì„œì— í¬í•¨: 12ê°œ (60%)
â†’ ì •ë³´ ê°€ì¹˜ê°€ ë†’ì€ ê²ƒë§Œ ì„ íƒ
```

#### 6.2.5 ì •ë³´ ì •ì œ (Polished Information)

**ì›ì¹™**: ìˆ«ì, ì‹œê°í™”, í…ìŠ¤íŠ¸ë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬

**ë³¸ í”„ë¡œì íŠ¸ ì ìš©**:

**1) ìˆ«ì í‘œí˜„**:
```markdown
# ë‚˜ìœ ì˜ˆ
Accuracy: 0.5951612903225806

# ì¢‹ì€ ì˜ˆ
Accuracy: 59.5%
```

**2) ì¹´í…Œê³ ë¦¬ ëª…ëª…**:
```markdown
# ë‚˜ìœ ì˜ˆ
MOS: 1, 2, 3, 4, 5

# ì¢‹ì€ ì˜ˆ
MOS: Bad, Poor, Fair, Good, Excellent
```

**3) ìƒ‰ìƒ ì½”ë”©**:
```markdown
# ë‚˜ìœ ì˜ˆ (í‘ë°±, ë‹¨ì¡°ë¡œì›€)
Baseline: 50.8%
Our model: 59.5%

# ì¢‹ì€ ì˜ˆ (ìƒ‰ìƒ + ì•„ì´ì½˜)
ğŸ”´ Baseline: 50.8% (must beat)
ğŸŸ¢ Gradient Boosting: 59.5% (best objective)
ğŸ”µ Random Forest (Full): 81.9% (best but leaky)
```

**4) ì‹œê°í™” í’ˆì§ˆ**:
- **DPI 300+**: ì¶œíŒ í’ˆì§ˆ (11/12ê°œ PNG ì¶©ì¡±)
- **í°íŠ¸ 12pt+**: ê°€ë…ì„±
- **ëª…í™•í•œ ë¼ë²¨**: X/Yì¶•, ì œëª©, ë²”ë¡€
- **ìƒ‰ë§¹ ì¹œí™”ì  íŒ”ë ˆíŠ¸**: viridis, Set2

### 6.3 ìµœì¢… ë³´ê³ ì„œ í‰ê°€

**ë³´ê³ ì„œ**: `Pokemon QoE Dataset Final Analysis Report_Changyong Hyun.pdf` (28í˜ì´ì§€)

**í‰ê°€ ê²°ê³¼: 97/100ì **

| ê¸°ì¤€ | ì ìˆ˜ | ê·¼ê±° |
|------|------|------|
| **Context ì œê³µ** | 100/100 | Pokemon í”„ë¡œì íŠ¸ ë°°ê²½, ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ìƒì„¸ ì„¤ëª… |
| **ë°ì´í„° ì´í•´** | 100/100 | í´ë˜ìŠ¤ ë¶ˆê· í˜•, Data Leakage ì¡°ê¸° ë°œê²¬ |
| **í”„ë¡œí† ì½œ ì„¤ëª…** | 100/100 | ëª¨ë“  ë‹¨ê³„ì— WHY í¬í•¨, Expected vs Actual ì²´ê³„ì  |
| **Expected vs Actual** | 100/100 | 4ê°œ ê°€ì„¤ ëª¨ë‘ ê²€ì¦, ê²°ê³¼ ë¹„êµ ëª…í™• |
| **ë¹„íŒì  í‰ê°€** | 100/100 | 5ê°€ì§€ í•œê³„ì  ì†”ì§íˆ ì¸ì •, ê³¼ì¥ ì—†ìŒ |
| **ì •ë³´ ì •ì œ** | 95/100 | 28í˜ì´ì§€ë¡œ ì•½ê°„ ê¸¸ì§€ë§Œ ëª¨ë‘ ì˜ë¯¸ìˆìŒ |
| **ìŠ¤í† ë¦¬í…”ë§** | 100/100 | ë…¼ë¦¬ì  íë¦„, ëª…í™•í•œ êµ¬ì¡°, ì‹¤ë¬´ì  ê¶Œì¥ì‚¬í•­ |

**ê°ì  ì´ìœ **:
- ê°€ì´ë“œë¼ì¸ì— "ë„ˆë¬´ ë§ì´ ì“°ì§€ ë§ ê²ƒ" (12-15í˜ì´ì§€ ê¶Œì¥)
- í•˜ì§€ë§Œ 28í˜ì´ì§€ ëª¨ë‘ ì˜ë¯¸ìˆëŠ” ë‚´ìš© (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ìŒ)
- ë”°ë¼ì„œ -5ì ë§Œ ê°ì 

**ê°•ì **:
- âœ… Data Leakage íƒì§€ ë° ì •ëŸ‰í™” (ëª¨ë²” ì‚¬ë¡€)
- âœ… í•œê³„ì  ì†”ì§ ì¸ì • (59.5% "ì œí•œì "ì´ë¼ê³  ëª…ì‹œ)
- âœ… DPI 300+ ì‹œê°í™” (11/12ê°œ)
- âœ… ì‹¤ë¬´ì  ê¶Œì¥ì‚¬í•­ (ë¬´ì—‡ì„ í•  ìˆ˜ ìˆê³  ì—†ëŠ”ì§€ ëª…í™•)

---

## 7. í•µì‹¬ êµí›ˆ

### 7.1 ë°ì´í„° ê³¼í•™ ê´€ì 

#### 7.1.1 Data Leakageê°€ ê°€ì¥ ìœ„í—˜

**êµí›ˆ**:
- ë†’ì€ ì„±ëŠ¥(81.9%)ì´ í•­ìƒ ì¢‹ì€ ê²ƒì€ ì•„ë‹˜
- ë°°í¬ ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸ì€ ë¬´ì˜ë¯¸
- 22.4%p ì„±ëŠ¥ ì°¨ì´ = ì‹¤íŒ¨í•œ í”„ë¡œì íŠ¸

**ì˜ˆë°© ë°©ë²•**:
1. **í”¼ì²˜ ì¶œì²˜ í™•ì¸**: ì´ ë°ì´í„°ë¥¼ ì‹¤ì œë¡œ ì–»ì„ ìˆ˜ ìˆë‚˜?
2. **ì‹œê°„ ìˆœì„œ**: ì˜ˆì¸¡ ì‹œì ì— í”¼ì²˜ê°€ ì¡´ì¬í•˜ë‚˜?
3. **íƒ€ê²Ÿ ìœ ë„**: í”¼ì²˜ê°€ íƒ€ê²Ÿìœ¼ë¡œë¶€í„° ìœ ë„ë˜ì—ˆë‚˜?
4. **ë¶„ë¦¬ í…ŒìŠ¤íŠ¸**: ì˜ì‹¬ í”¼ì²˜ ì œì™¸ ì‹œ ì„±ëŠ¥ ê¸‰ë½í•˜ë‚˜?

**ì‹¤ë¬´ ì‚¬ë¡€**:
```python
# ğŸ”´ Leakage ì˜ˆì‹œ
features = [
    'buffering_time',        # âœ… OK (ì˜ˆì¸¡ ì‹œì ì— ì•Œ ìˆ˜ ìˆìŒ)
    'network_type',          # âœ… OK
    'user_satisfaction',     # âŒ Leakage! (íƒ€ê²Ÿê³¼ ê°™ì€ ê°œë…)
    'future_churn'           # âŒ Leakage! (ë¯¸ë˜ ì •ë³´)
]
```

#### 7.1.2 í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ì‹¬ê°í•œ ë¬¸ì œ

**êµí›ˆ**:
- Accuracy ë‹¨ë… ì‚¬ìš© ê¸ˆì§€
- Baselineì´ 50.8%ë©´ ëª¨ë¸ í‰ê°€ ì‹ ì¤‘íˆ
- ì†Œìˆ˜ í´ë˜ìŠ¤ê°€ ì¢…ì¢… ê°€ì¥ ì¤‘ìš” (ë¶ˆë§Œì¡± ì‚¬ìš©ì ì¡°ê¸° ë°œê²¬)

**ëŒ€ì‘ ì „ëµ** (ìš°ì„ ìˆœìœ„ìˆœ):
1. **Stratified Split**: í•„ìˆ˜
2. **Class Weights**: `class_weight='balanced'`
3. **ëŒ€ì•ˆ ì§€í‘œ**: F1-Score, Cohen's Kappa, ROC-AUC
4. **SMOTE**: í•©ì„± ìƒ˜í”Œ ìƒì„±
5. **Threshold Tuning**: í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ìµœì í™”
6. **Ensemble**: ì—¬ëŸ¬ ëª¨ë¸ ê²°í•©

#### 7.1.3 Feature Engineeringì´ ì¤‘ìš”

**êµí›ˆ**:
- ë„ë©”ì¸ ì§€ì‹ + ì°½ì˜ì„± = ì„±ëŠ¥ í–¥ìƒ
- Buffering_Severity (5ìœ„), Video_Quality_Index (3ìœ„) ëª¨ë‘ ìƒìœ„ ì§„ì…
- Raw í”¼ì²˜ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±

**ì„±ê³µ ë¹„ê²°**:
1. **ë„ë©”ì¸ ì´í•´**: ë²„í¼ë§ì˜ ë³µí•© íš¨ê³¼ (íšŸìˆ˜ Ã— ì‹œê°„)
2. **ìˆ˜í•™ì  ë³€í™˜**: ë¡œê·¸ ë³€í™˜ìœ¼ë¡œ ì •ê·œë¶„í¬ ê·¼ì‚¬
3. **ì°¨ì› ì¶•ì†Œ**: 5ê°œ ë„¤íŠ¸ì›Œí¬ íƒ€ì… â†’ 3ê°œ ì„¸ëŒ€
4. **ê°€ì¤‘ ê²°í•©**: ì—¬ëŸ¬ í’ˆì§ˆ ì§€í‘œë¥¼ í•˜ë‚˜ë¡œ

**ì‹¤ë¬´ íŒ**:
```python
# ë‹¨ìˆœ ì›ë³¸ í”¼ì²˜
features = ['bitrate', 'framerate', 'resolution']

# Feature Engineering
features = [
    'bitrate',
    'framerate',
    'resolution',
    'video_quality_index',  # = 0.4Ã—bitrate + 0.3Ã—framerate + 0.3Ã—resolution
    'bitrate_log',          # ë¡œê·¸ ë³€í™˜
    'bitrate_per_resolution' # êµí˜¸ì‘ìš©
]
```

#### 7.1.4 ê³¼ì í•©ì€ ì‹¤íŒ¨ì˜ ì‹ í˜¸

**êµí›ˆ**:
- Train 99%, Test 59% = ì‹¤íŒ¨
- ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ë” ë‚®ì„ ê°€ëŠ¥ì„±
- ì¼ë°˜í™”ê°€ ìµœì¢… ëª©í‘œ

**ì§„ë‹¨**:
```python
overfit_gap = train_accuracy - test_accuracy

if overfit_gap < 0.05:
    print("âœ… ì–‘í˜¸")
elif overfit_gap < 0.15:
    print("âš ï¸ ì£¼ì˜")
elif overfit_gap < 0.30:
    print("ğŸ”´ ê³¼ì í•© ìš°ë ¤")
else:
    print("ğŸš¨ ì‹¬ê°í•œ ê³¼ì í•©")  # â† ìš°ë¦¬ í”„ë¡œì íŠ¸ (39.6%)
```

**í•´ê²°ì±…**:
1. **ë” ë§ì€ ë°ì´í„°**: ê°€ì¥ íš¨ê³¼ì 
2. **ì •ê·œí™”**: L1/L2, Dropout, Pruning
3. **Cross-Validation**: K-Fold (k=5 ë˜ëŠ” 10)
4. **ì¡°ê¸° ì¢…ë£Œ**: Validation loss ì¦ê°€ ì‹œ ì¤‘ë‹¨
5. **Hyperparameter Tuning**: GridSearchCV

### 7.2 í•™ìˆ ì  ê´€ì 

#### 7.2.1 WHY ì¤‘ì‹¬ ì‚¬ê³ 

**êµí›ˆ**:
- "ì´ê±¸ í–ˆë‹¤" (X) â†’ "ì™œ ì´ê±¸ í–ˆê³ , ê²°ê³¼ê°€ ì–´ë• ë‚˜" (O)
- Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ì˜ í•µì‹¬

**ì ìš© ì˜ˆì‹œ**:
```markdown
# ë‚˜ìœ ì˜ˆ
"We split the data 80/20."

# ì¢‹ì€ ì˜ˆ
"WHY: To ensure unbiased evaluation, we need separate train/test sets.
The 80/20 split provides sufficient training data (1,234 samples) while
maintaining a reasonable test set (309 samples) for statistical significance.

WHAT: We used train_test_split with test_size=0.2, random_state=42,
and stratify=y to preserve class distribution."
```

#### 7.2.2 í•œê³„ì  ì¸ì •ì˜ ì¤‘ìš”ì„±

**êµí›ˆ**:
- ì†”ì§í•¨ > ê³¼ì¥
- 59.5%ë¥¼ "ìš°ìˆ˜"ë¡œ í¬ì¥ (X) â†’ "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ë‚˜ ì‹¤ìš©ì„± ì œí•œì " (O)
- í•™ìˆ ì  ì‹ ë¢°ë„ â†‘

**ì‹¤ì œ ì˜í–¥**:
- ë³¸ í”„ë¡œì íŠ¸ ë³´ê³ ì„œ: 97/100ì 
- í•œê³„ì  ì¸ì •ì´ ê°ì ì´ ì•„ë‹ˆë¼ ê°€ì !

#### 7.2.3 ì¬í˜„ ê°€ëŠ¥ì„± (Reproducibility)

**êµí›ˆ**:
- `random_state=42` ì‚¬ìš©
- ëª¨ë“  ì „ì²˜ë¦¬ ë‹¨ê³„ ë¬¸ì„œí™”
- Scaler/ëª¨ë¸ ì €ì¥
- ë‹¤ë¥¸ ì—°êµ¬ìê°€ ì •í™•íˆ ì¬í˜„ ê°€ëŠ¥

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
```python
âœ… random_state ê³ ì •
âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ëª…ì‹œ (requirements.txt)
âœ… ë°ì´í„° ë¶„í•  ì¬í˜„ ê°€ëŠ¥
âœ… Scaler ì €ì¥ (ì‹¤ì œ ë°°í¬ ì‹œ í•„ìˆ˜)
âœ… ëª¨ë¸ ì €ì¥
âœ… ë…¸íŠ¸ë¶ ì‹¤í–‰ ìˆœì„œ ëª…í™•
âœ… í™˜ê²½ ì„¤ì • ë¬¸ì„œí™”
```

### 7.3 ì‹¤ë¬´ì  ê´€ì 

#### 7.3.1 ë°°í¬ ì‹œ í˜„ì‹¤ ê³ ë ¤

**êµí›ˆ**:
- 2015ë…„ ë°ì´í„° â†’ 2024ë…„ ë°°í¬ = ìœ„í—˜
- ì§€ì—­/ì¸êµ¬ íŠ¹í™” â†’ ì¼ë°˜í™” ì–´ë ¤ì›€
- ë„ë©”ì¸ ì „ì´ (Domain Adaptation) í•„ìˆ˜

**ì‹¤ë¬´ ì „ëµ**:
1. **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**:
   - Model drift íƒì§€
   - ì„±ëŠ¥ ì§€í‘œ ì‹¤ì‹œê°„ ì¶”ì 
2. **ì •ê¸° ì¬í•™ìŠµ**:
   - ë¶„ê¸°ë³„ ë˜ëŠ” ë°˜ê¸°ë³„
   - ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
3. **A/B í…ŒìŠ¤íŒ…**:
   - ì‹ ëª¨ë¸ vs êµ¬ëª¨ë¸
   - ì ì§„ì  ë¡¤ì•„ì›ƒ
4. **Fallback ë©”ì»¤ë‹ˆì¦˜**:
   - ì„±ëŠ¥ ì €í•˜ ì‹œ ë£° ê¸°ë°˜ìœ¼ë¡œ ì „í™˜

#### 7.3.2 ì ì ˆí•œ í™œìš©

**êµí›ˆ**:
- ê³ ìœ„í—˜ ìë™í™” (X)
- íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§/ì¡°ê¸° ê²½ê³  (O)
- "ì‚¬ëŒ ë³´ì¡°"ë¡œ ì‹œì‘

**ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬**:
```
ëª¨ë¸ ì‹ ë¢°ë„ (Confidence) vs ê²°ì • ì˜í–¥ (Impact)

High Impact:
- ë†’ì€ ì‹ ë¢°ë„ í•„ìš” (Îº > 0.6, Acc > 90%)
- ì˜ˆ: SLA í™˜ë¶ˆ, ë„¤íŠ¸ì›Œí¬ ì¬êµ¬ì„±
- ìš°ë¦¬ ëª¨ë¸: âŒ ë¶€ì í•© (Îº=0.335, Acc=59.5%)

Low Impact:
- ë‚®ì€ ì‹ ë¢°ë„ í—ˆìš© (Îº > 0.2, Acc > 55%)
- ì˜ˆ: íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§, ì¡°ê¸° ê²½ê³ 
- ìš°ë¦¬ ëª¨ë¸: âœ… ì í•©
```

#### 7.3.3 ì§€ì†ì  ê°œì„  í•„ìš”

**êµí›ˆ**:
- ì²« ë²ˆì§¸ ëª¨ë¸ì€ ì‹œì‘ì¼ ë¿
- 59.5% â†’ 70% â†’ 80% (ì‹¤ì œ ë°°í¬ ê°€ëŠ¥)
- MLOps íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**ë¡œë“œë§µ**:
```
Phase 1 (í˜„ì¬): 59.5% accuracy
- Baseline êµ¬ì¶•
- Data Leakage íƒì§€
- í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë°œê²¬

Phase 2 (ë‹¨ê¸°, 3ê°œì›”):
- SMOTE ì ìš© â†’ 5~10%p í–¥ìƒ ì˜ˆìƒ
- Hyperparameter Tuning â†’ 2~5%p í–¥ìƒ
- XGBoost ì‹œë„ â†’ 1~3%p í–¥ìƒ
- ëª©í‘œ: 70% accuracy

Phase 3 (ì¤‘ê¸°, 6ê°œì›”):
- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ (10,000+ ìƒ˜í”Œ)
- ìµœì‹  ë„¤íŠ¸ì›Œí¬ (5G) í¬í•¨
- ë”¥ëŸ¬ë‹ ì‹œë„ (LSTM for temporal patterns)
- ëª©í‘œ: 80% accuracy

Phase 4 (ì¥ê¸°, 1ë…„):
- MLOps íŒŒì´í”„ë¼ì¸ (ìë™ ì¬í•™ìŠµ)
- Model drift ëª¨ë‹ˆí„°ë§
- Multi-modal (ë¹„ë””ì˜¤ + ë„¤íŠ¸ì›Œí¬ + ì‚¬ìš©ì í–‰ë™)
- ëª©í‘œ: 85%+ accuracy, ì‹¤ì œ ë°°í¬
```

---

## 8. í–¥í›„ ê°œì„  ë°©í–¥

### 8.1 ìš°ì„ ìˆœìœ„ ë†’ìŒ (High Impact, Quick Wins)

#### 8.1.1 SMOTE ì ìš©
**ëª©ì **: ì†Œìˆ˜ í´ë˜ìŠ¤ í•©ì„± ìƒ˜í”Œ ìƒì„±ìœ¼ë¡œ ë¶ˆê· í˜• í•´ê²°

**ë°©ë²•**:
```python
from imblearn.over_sampling import SMOTE

# SMOTE ì ìš©
smote = SMOTE(random_state=42, k_neighbors=5)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# ê²°ê³¼ í™•ì¸
print("Before SMOTE:")
print(y_train.value_counts())
# 1: 74, 2: 94, 3: 196, 4: 627, 5: 243

print("\nAfter SMOTE:")
print(pd.Series(y_train_resampled).value_counts())
# 1: 627, 2: 627, 3: 627, 4: 627, 5: 627 (ê· í˜•!)
```

**ì˜ˆìƒ íš¨ê³¼**:
- MOS=1,2 ì¬í˜„ìœ¨: 40% â†’ 60~70%
- ì „ì²´ F1-Score: 0.552 â†’ 0.60~0.65
- Accuracy: 59.5% â†’ 63~68%

**ì£¼ì˜ì‚¬í•­**:
- Test setì—ëŠ” SMOTE ì ìš© ê¸ˆì§€ (Trainë§Œ!)
- Overfitting ì¦ê°€ ê°€ëŠ¥ â†’ Cross-validation í•„ìˆ˜

#### 8.1.2 Hyperparameter Tuning
**ëª©ì **: ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ ë° ê³¼ì í•© ê°ì†Œ

**ë°©ë²•**:
```python
from sklearn.model_selection import GridSearchCV

# Gradient Boosting íŠœë‹
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_samples_split': [10, 20, 50],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid,
    cv=5,                  # 5-Fold Cross-Validation
    scoring='f1_macro',    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤
    n_jobs=-1
)

grid_search.fit(X_train_scaled, y_train)
print(f"Best params: {grid_search.best_params_}")
print(f"Best CV F1: {grid_search.best_score_:.3f}")
```

**ì˜ˆìƒ íš¨ê³¼**:
- Overfit Gap: 39.6% â†’ 20~25%
- Test Accuracy: 59.5% â†’ 61~64%
- ì¼ë°˜í™” ì„±ëŠ¥ â†‘

#### 8.1.3 Ensemble/Stacking
**ëª©ì **: ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë” robustí•œ ì˜ˆì¸¡

**ë°©ë²•**:
```python
from sklearn.ensemble import VotingClassifier, StackingClassifier

# Voting (ë‹¤ìˆ˜ê²°)
voting_clf = VotingClassifier(
    estimators=[
        ('gb', GradientBoostingClassifier(...)),
        ('rf', RandomForestClassifier(...)),
        ('lr', LogisticRegression(...))
    ],
    voting='soft'  # í™•ë¥  ê¸°ë°˜ íˆ¬í‘œ
)

# Stacking (ë©”íƒ€ ëª¨ë¸)
stacking_clf = StackingClassifier(
    estimators=[
        ('gb', GradientBoostingClassifier(...)),
        ('rf', RandomForestClassifier(...)),
    ],
    final_estimator=LogisticRegression(),
    cv=5
)
```

**ì˜ˆìƒ íš¨ê³¼**:
- Test Accuracy: 59.5% â†’ 62~66%
- ê·¹ë‹¨ì  ì˜¤ë¥˜ ê°ì†Œ (ì—¬ëŸ¬ ëª¨ë¸ì´ ë™ì˜í•  ë•Œë§Œ ì˜ˆì¸¡)

### 8.2 ìš°ì„ ìˆœìœ„ ì¤‘ê°„ (Medium Impact)

#### 8.2.1 XGBoost ì‹œë„
**ëª©ì **: Gradient Boostingì˜ ê³ ê¸‰ ë²„ì „, ë” ë‚˜ì€ ì •ê·œí™”

**ë°©ë²•**:
```python
from xgboost import XGBClassifier

xgb_clf = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    min_child_weight=3,
    gamma=0.1,              # ì¶”ê°€ ì •ê·œí™”
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,          # L1 ì •ê·œí™”
    reg_lambda=1.0,         # L2 ì •ê·œí™”
    random_state=42
)
```

**ì˜ˆìƒ íš¨ê³¼**:
- Overfit Gap ê°ì†Œ: 39.6% â†’ 25~30%
- Test Accuracy: 59.5% â†’ 60~63%

#### 8.2.2 Threshold Optimization
**ëª©ì **: í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì„ê³„ê°’ ìµœì í™”

**ë°©ë²•**:
```python
# ê¸°ë³¸: P(MOS=5) > 0.5 â†’ ì˜ˆì¸¡ 5
# ìµœì í™”: P(MOS=5) > 0.3 â†’ ì˜ˆì¸¡ 5 (ì¬í˜„ìœ¨ â†‘)

from sklearn.metrics import precision_recall_curve

# ê° í´ë˜ìŠ¤ë³„ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
for cls in [1, 2, 3, 4, 5]:
    y_prob = model.predict_proba(X_test)[:, cls-1]
    precision, recall, thresholds = precision_recall_curve(
        y_test == cls, y_prob
    )
    # F1 ìµœëŒ€í™” ì„ê³„ê°’ ì„ íƒ
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    best_threshold = thresholds[np.argmax(f1_scores)]
    print(f"MOS={cls}: optimal threshold = {best_threshold:.3f}")
```

**ì˜ˆìƒ íš¨ê³¼**:
- MOS=5 ì¬í˜„ìœ¨: 25% â†’ 40~50%
- ì „ì²´ F1-Score: 0.552 â†’ 0.57~0.60

#### 8.2.3 Advanced Feature Engineering
**ëª©ì **: êµí˜¸ì‘ìš© í•­ ì¶”ê°€

**ë°©ë²•**:
```python
# êµí˜¸ì‘ìš© í”¼ì²˜
df['buffering_x_network'] = df['Buffering_Severity'] * df['Network_Generation']
df['bitrate_x_resolution'] = df['QoA_VLCbitrate'] * df['QoA_VLCresolution']
df['age_x_buffering'] = df['QoU_age'] * df['QoA_BUFFERINGcount']

# ë‹¤í•­ì‹ í”¼ì²˜
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_poly = poly.fit_transform(X)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë¹„ì„ í˜• ê´€ê³„ ë” ì˜ í¬ì°©
- Test Accuracy: 59.5% â†’ 61~64%

**ì£¼ì˜**: ê³¼ì í•© ìœ„í—˜ ì¦ê°€ â†’ ì •ê·œí™” í•„ìˆ˜

### 8.3 ì¥ê¸°ì  ê°œì„  (Long-term, High Investment)

#### 8.3.1 ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘
**ëª©ì **: ê³¼ì í•© í•´ê²°ì˜ ê·¼ë³¸ì  ë°©ë²•

**ëª©í‘œ**:
- í˜„ì¬: 1,543 ìƒ˜í”Œ
- ëª©í‘œ: 10,000+ ìƒ˜í”Œ

**ìˆ˜ì§‘ ì „ëµ**:
1. **ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬**: 2G~5G, ë‹¤ì–‘í•œ í†µì‹ ì‚¬
2. **ë‹¤ì–‘í•œ ì§€ì—­**: í”„ë‘ìŠ¤ ì™¸ ìœ ëŸ½/ì•„ì‹œì•„/ë¯¸êµ­
3. **ë‹¤ì–‘í•œ ì¸êµ¬**: 10ëŒ€~70ëŒ€, ë‹¤ì–‘í•œ ì§ì—…
4. **ë‹¤ì–‘í•œ ì½˜í…ì¸ **: ì˜í™”, ìŠ¤í¬ì¸ , ë‰´ìŠ¤, êµìœ¡
5. **ìµœì‹  ê¸°ìˆ **: H.265/VP9 ì½”ë±, 4K í•´ìƒë„

**ì˜ˆìƒ íš¨ê³¼**:
- Overfit Gap: 39.6% â†’ 10~15%
- Test Accuracy: 59.5% â†’ 70~75%
- ì¼ë°˜í™” ì„±ëŠ¥ â†‘â†‘

#### 8.3.2 MLOps íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
**ëª©ì **: ìë™ ì¬í•™ìŠµ, ëª¨ë¸ drift ëª¨ë‹ˆí„°ë§

**ì•„í‚¤í…ì²˜**:
```
ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ í•™ìŠµ â†’ í‰ê°€ â†’ ë°°í¬ â†’ ëª¨ë‹ˆí„°ë§
     â†‘                                            â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¬í•™ìŠµ íŠ¸ë¦¬ê±° â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë„êµ¬**:
- **MLflow**: ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- **Airflow**: íŒŒì´í”„ë¼ì¸ ìŠ¤ì¼€ì¤„ë§
- **Prometheus + Grafana**: ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **Docker + Kubernetes**: ì»¨í…Œì´ë„ˆí™” ë°°í¬

**ê¸°ëŠ¥**:
1. **ìë™ ì¬í•™ìŠµ**: ì£¼ê°„/ì›”ê°„ ìŠ¤ì¼€ì¤„
2. **Model Drift íƒì§€**: ì„±ëŠ¥ ì €í•˜ ì‹œ ì•Œë¦¼
3. **A/B í…ŒìŠ¤íŒ…**: ì‹ ëª¨ë¸ vs êµ¬ëª¨ë¸ ë¹„êµ
4. **ë¡¤ë°±**: ì„±ëŠ¥ ì €í•˜ ì‹œ ìë™ ì´ì „ ë²„ì „ ë³µêµ¬

#### 8.3.3 ë”¥ëŸ¬ë‹ ì‹œë„
**ëª©ì **: ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ í¬ì°©

**ì•„í‚¤í…ì²˜**:
```python
import tensorflow as tf

# Simple MLP
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_dim=19),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')  # 5 classes
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

**ê³ ê¸‰ ì ‘ê·¼**:
- **LSTM**: ì‹œê°„ì  íŒ¨í„´ (ì‚¬ìš©ìì˜ ê³¼ê±° ì„¸ì…˜ ê³ ë ¤)
- **Attention**: ì¤‘ìš”í•œ í”¼ì²˜ì— ì§‘ì¤‘
- **Multi-modal**: ë¹„ë””ì˜¤ + ë„¤íŠ¸ì›Œí¬ + ì‚¬ìš©ì í–‰ë™ ê²°í•©

**ì˜ˆìƒ íš¨ê³¼**:
- Test Accuracy: 59.5% â†’ 75~80% (ì¶©ë¶„í•œ ë°ì´í„° ì‹œ)
- í•˜ì§€ë§Œ í•´ì„ ê°€ëŠ¥ì„± â†“

---

## 9. ê²°ë¡ 

### 9.1 í”„ë¡œì íŠ¸ ìš”ì•½

**ì´ í”„ë¡œì íŠ¸ëŠ”**:
- âœ… ì™„ì „í•œ ë°ì´í„° ê³¼í•™ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰ (ì´í•´ â†’ íƒìƒ‰ â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë§)
- âœ… ì—„ê²©í•œ ë°©ë²•ë¡  ì¤€ìˆ˜ (Alessandro Maddaloni ê°€ì´ë“œë¼ì¸)
- âœ… Data Leakage íƒì§€ ë° ì •ëŸ‰í™” (22.4%p ì°¨ì´)
- âœ… ì†”ì§í•œ í•œê³„ì  ì¸ì • (5ê°€ì§€ í•œê³„ì )
- âœ… ì‹¤ë¬´ì  ê¶Œì¥ì‚¬í•­ ì œì‹œ (ë¬´ì—‡ì„ í•  ìˆ˜ ìˆê³  ì—†ëŠ”ì§€)

### 9.2 ìµœì¢… í‰ê°€

**59.5% ëª¨ë¸ (Gradient Boosting, Objective)**:
- âœ… í†µê³„ì ìœ¼ë¡œ ìœ ì˜ (Baseline 50.8% ëŒ€ë¹„ +8.7%p, Îº=0.335)
- âš ï¸ í•˜ì§€ë§Œ ì‹¤ìš©ì„± ì œí•œì  (40.5% ì—ëŸ¬ìœ¨)
- âš ï¸ ì‹¬ê°í•œ ê³¼ì í•© (39.6% Train-Test gap)
- ğŸ¯ **ì í•©í•œ ìš©ë„**: íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§, A/B í…ŒìŠ¤íŒ…, ì¡°ê¸° ê²½ê³  ì‹œìŠ¤í…œ
- âŒ **ë¶€ì í•©í•œ ìš©ë„**: ê³ ìœ„í—˜ ìë™í™” ê²°ì • (SLA í™˜ë¶ˆ, ë„¤íŠ¸ì›Œí¬ ì¬êµ¬ì„±)

**81.9% ëª¨ë¸ (Random Forest, Full)**:
- âœ… ìš°ìˆ˜í•œ ì„±ëŠ¥ (Îº=0.727 "substantial agreement")
- âŒ í•˜ì§€ë§Œ Data Leakageë¡œ ë°°í¬ ë¶ˆê°€
- ğŸ¯ **ê°€ì¹˜**: Data Leakage ì •ëŸ‰í™”, ë°©ë²•ë¡ ì  êµí›ˆ

### 9.3 ê°€ì¥ í° ê¸°ì—¬

**1) ë²„í¼ë§ì´ QoEì˜ í•µì‹¬ ìš”ì¸**:
- í”¼ì²˜ ì¤‘ìš”ë„ 36.6% ì°¨ì§€
- ìƒê´€ê³„ìˆ˜ r=-0.482
- ì‚¬ìš©ì í—ˆìš© ì„ê³„ê°’: 2íšŒ (ì‹¤ë¬´ì  ì¸ì‚¬ì´íŠ¸)

**2) Data Leakageì˜ ëª¨ë²” ì‚¬ë¡€**:
- íƒì§€ (EDAì—ì„œ r=0.841)
- ì •ëŸ‰í™” (Objective vs Full: 22.4%p)
- ëŒ€ì‘ (ë³„ë„ ë°ì´í„°ì…‹ êµ¬ì¶•)

**3) ë°©ë²•ë¡ ì  ì—„ê²©í•¨**:
- Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ 97/100ì 
- WHY before WHAT, Expected vs Actual, Critical Assessment

### 9.4 ì‹¤ë¬´ í™œìš©

**ì¦‰ì‹œ í™œìš© ê°€ëŠ¥** âœ…:
```python
# íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§
if weekly_avg_mos.diff() < -0.2:
    alert("QoE ì €í•˜ íŠ¸ë Œë“œ ê°ì§€")

# A/B í…ŒìŠ¤íŒ…
if treatment_mos > control_mos + 0.15:
    print("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê°œì„ ")

# ì¡°ê¸° ê²½ê³  (ì‚¬ëŒ ê²€í† ìš©)
if predicted_mos < 3.0:
    flag_for_review()
```

**ë¯¸ë˜ ëª©í‘œ** (ë°ì´í„° ìˆ˜ì§‘ + ê°œì„  í›„) ğŸ¯:
```python
# ëª©í‘œ: 80%+ accuracy, Îº > 0.7
# ê°€ëŠ¥í•œ ê²ƒ:
- ì‹¤ì‹œê°„ QoE ì˜ˆì¸¡ ë° ë„¤íŠ¸ì›Œí¬ ìµœì í™”
- ì‚¬ìš©ìë³„ ë§ì¶¤í˜• ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
- SLA ìë™ ê´€ë¦¬
```

### 9.5 í•™ìˆ ì  ê°€ì¹˜

**1) êµìœ¡ ìë£Œ**:
- Data Leakage ê²½ê³„ ì‚¬ë¡€
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘
- Feature Engineering ì‚¬ë¡€
- Alessandro Maddaloni ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ ëª¨ë²”

**2) ë°©ë²•ë¡ ì  ê¸°ì—¬**:
- WHY ì¤‘ì‹¬ ì‚¬ê³ ì˜ ì¤‘ìš”ì„±
- í•œê³„ì  ì¸ì •ì˜ í•™ìˆ ì  ê°€ì¹˜
- ì¬í˜„ ê°€ëŠ¥ì„± (reproducibility) êµ¬í˜„

**3) ë„ë©”ì¸ ì§€ì‹**:
- QoE ì˜ˆì¸¡ì˜ í•œê³„ (ì£¼ê´€ì„±)
- ë²„í¼ë§ í—ˆìš© ì„ê³„ê°’ (2íšŒ)
- ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ìš”êµ¬ì‚¬í•­ (3G ì´ìƒ)

### 9.6 ë§ˆì§€ë§‰ ë©”ì‹œì§€

**ë°ì´í„° ê³¼í•™ìì—ê²Œ**:
> "ë†’ì€ ì„±ëŠ¥(81.9%)ë³´ë‹¤ ì˜¬ë°”ë¥¸ ë°©ë²•ë¡ (59.5%)ì´ ë” ê°€ì¹˜ìˆë‹¤. Data LeakageëŠ” ë…¼ë¬¸ì„ ë§ì¹˜ì§€ë§Œ, ì´ë¥¼ íƒì§€í•˜ê³  ì •ëŸ‰í™”í•˜ëŠ” ê²ƒì€ ë…¼ë¬¸ì„ ì™„ì„±í•œë‹¤."

**ì‹¤ë¬´ìì—ê²Œ**:
> "59.5% ëª¨ë¸ë„ ê°€ì¹˜ìˆë‹¤. íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§ê³¼ ì¡°ê¸° ê²½ê³ ë¡œ ì‹œì‘í•˜ê³ , ë°ì´í„°ë¥¼ ëª¨ìœ¼ë©°, ì ì§„ì ìœ¼ë¡œ ê°œì„ í•˜ë¼. ì™„ë²½í•œ ëª¨ë¸ì„ ê¸°ë‹¤ë¦¬ë‹¤ ì•„ë¬´ê²ƒë„ ë°°í¬í•˜ì§€ ëª»í•˜ëŠ” ê²ƒë³´ë‹¤ ë‚«ë‹¤."

**í•™ìƒì—ê²Œ**:
> "ì´ í”„ë¡œì íŠ¸ëŠ” ë°ì´í„° ê³¼í•™ì˜ ì „ì²´ ê³¼ì •ì„ ë‹´ì•˜ë‹¤. ì„±ê³µ(ë²„í¼ë§ ë°œê²¬)ê³¼ ì‹¤íŒ¨(ê³¼ì í•©), ê°€ì„¤ ê²€ì¦(4ê°œ), ê·¸ë¦¬ê³  ì†”ì§í•œ í•œê³„ ì¸ì •. ì´ê²ƒì´ ì§„ì§œ ë°ì´í„° ê³¼í•™ì´ë‹¤."

---

**ì‘ì„±ì**: Changyong Hyun
**ê³¼ëª©**: Data Science - Theory to Practice
**ê¸°ê´€**: Telecom SudParis, Institut Polytechnique de Paris
**ì™„ë£Œì¼**: 2024ë…„ 10ì›” 13ì¼
**ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ë„**: 97/100ì 
**í”„ë¡œì íŠ¸ ì½”ë“œ**: https://github.com/[your-repo]/pokemon-qoe-dataset

---

## ë¶€ë¡

### A. ìš©ì–´ ì •ì˜

- **MOS (Mean Opinion Score)**: ì‚¬ìš©ì ë§Œì¡±ë„ 5ì  ì²™ë„ (1=Bad ~ 5=Excellent)
- **QoE (Quality of Experience)**: ì‚¬ìš©ì ê²½í—˜ í’ˆì§ˆ
- **Data Leakage**: ì‹¤ì œ ë°°í¬ ì‹œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ì •ë³´ê°€ í•™ìŠµ ë°ì´í„°ì— í¬í•¨ë˜ëŠ” í˜„ìƒ
- **Class Imbalance**: í´ë˜ìŠ¤ ë¶„í¬ê°€ ë¶ˆê· í˜•í•œ ìƒíƒœ (MOS=4ê°€ 50.8%)
- **Overfit Gap**: Trainê³¼ Test ì •í™•ë„ ì°¨ì´ (ê³¼ì í•© ì§€í‘œ)
- **Cohen's Kappa**: ìš°ì—° ì¼ì¹˜ë¥¼ ë³´ì •í•œ ì¼ì¹˜ë„ ì§€í‘œ (ì„œì—´ ë°ì´í„°ì— ì í•©)

### B. ê¸°ìˆ  ìŠ¤íƒ

**ì–¸ì–´ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬**:
- Python 3.8+
- NumPy, Pandas (ë°ì´í„° ì²˜ë¦¬)
- Scikit-learn (ë¨¸ì‹ ëŸ¬ë‹)
- Matplotlib, Seaborn (ì‹œê°í™”)
- Jupyter Notebook (ë¶„ì„ í™˜ê²½)

**ëª¨ë¸**:
- Logistic Regression
- Decision Tree
- Random Forest
- Gradient Boosting

**í‰ê°€ ì§€í‘œ**:
- Accuracy, F1-Score (Macro), Cohen's Kappa

### C. ì°¸ê³  ë¬¸í—Œ

1. **Pokemon í”„ë¡œì íŠ¸**: Paris Est CrÃ©teil University, LiSSi Lab
2. **Alessandro Maddaloni ê°€ì´ë“œë¼ì¸**: Telecom SudParis, Data Science Course 2024-2025
3. **ITU-T P.10**: "Vocabulary for performance, quality of service and quality of experience"
4. **Cookiecutter Data Science**: https://drivendata.github.io/cookiecutter-data-science/

### D. ì—°ë½ì²˜

í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜:
- ì´ë©”ì¼: [your-email]
- GitHub: [your-github]
- LinkedIn: [your-linkedin]

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024ë…„ 10ì›” 13ì¼
**ë¼ì´ì„¼ìŠ¤**: MIT License

---

**ë**
