# Pokemon QoE Dataset - Analysis Project

## Project Overview

This project analyzes the Pokemon Quality of Experience (QoE) dataset to predict user satisfaction (MOS - Mean Opinion Score) from objective network and video quality metrics. The dataset was collected during a crowdsourcing campaign where users watched videos on mobile devices across different network conditions.

**Goal:** Build predictive models to estimate QoE without requiring subjective user surveys, enabling proactive network optimization.

---

## Dataset Information

### Context
- **Source:** Pokemon Project (Platform Quality Evaluation of Mobile Networks)
- **Samples:** 1,560 video viewing sessions
- **Testers:** 181 participants (researchers and students, ages 19-38)
- **Location:** LiSSi laboratory, Paris, France
- **Collection Method:** Android application with VLC media player

### Features (23 total)

#### QoA - Video Quality Metrics (8 features)
- `QoA_VLCresolution`: Video resolution (240p, 360p)
- `QoA_VLCbitrate`: Video bitrate
- `QoA_VLCframerate`: Frame rate
- `QoA_VLCdropped`: Dropped frames
- `QoA_VLCaudiorate`: Audio bitrate
- `QoA_VLCaudioloss`: Audio packet loss
- `QoA_BUFFERINGcount`: Number of buffering events
- `QoA_BUFFERINGtime`: Total buffering time (ms)

#### QoS - Network Information (2 features)
- `QoS_type`: Network type (EDGE, UMTS, HSPA, HSPAP, LTE)
- `QoS_operator`: Mobile operator (SFR, BOUYEGUES, ORANGE, FREE)

#### QoD - Device Characteristics (3 features)
- `QoD_model`: Device model
- `QoD_os-version`: Android OS version
- `QoD_api-level`: Android API level

#### QoU - User Profile (3 features)
- `QoU_sex`: Gender (0=Female, 1=Male)
- `QoU_age`: Age
- `QoU_Ustedy`: Education level (1-5)

#### QoF - User Feedback (4 features)
- `QoF_begin`: Session start time
- `QoF_shift`: Time shift
- `QoF_audio`: Audio quality feedback
- `QoF_video`: Video quality feedback

#### Target Variable
- **MOS**: Mean Opinion Score (1=Bad, 2=Poor, 3=Fair, 4=Good, 5=Excellent)

### Class Distribution
- MOS 1 (Bad): 92 samples (5.9%)
- MOS 2 (Poor): 119 samples (7.6%)
- MOS 3 (Fair): 244 samples (15.6%)
- MOS 4 (Good): 787 samples (50.4%) âš ï¸ **Majority class**
- MOS 5 (Excellent): 300 samples (19.2%)

---

## Project Structure

```
Poqemon-QoE-Dataset-master/
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ config.py              # Central project configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Original dataset files
â”‚   â”‚   â”œâ”€â”€ pokemon.csv
â”‚   â”‚   â”œâ”€â”€ pokemon.arff
â”‚   â”‚   â”œâ”€â”€ pokemon.data
â”‚   â”‚   â””â”€â”€ pokemon.names
â”‚   â”œâ”€â”€ processed/             # Preprocessed data (train/test splits)
â”‚   â”‚   â”œâ”€â”€ X_train_full_scaled.csv
â”‚   â”‚   â”œâ”€â”€ X_test_full_scaled.csv
â”‚   â”‚   â”œâ”€â”€ X_train_objective_scaled.csv
â”‚   â”‚   â”œâ”€â”€ X_test_objective_scaled.csv
â”‚   â”‚   â””â”€â”€ y_*.csv files
â”‚   â””â”€â”€ external/              # External data (if any)
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ README.md              # Documentation navigation
â”‚   â”œâ”€â”€ PROJECT_GUIDE.md       # This file
â”‚   â”œâ”€â”€ SETUP_GUIDE.md         # Installation instructions
â”‚   â”œâ”€â”€ COMPLETION_SUMMARY.md  # Project completion report
â”‚   â”œâ”€â”€ DATASET_DESCRIPTION.md # Original dataset information
â”‚   â”œâ”€â”€ GUIDELINES_KR.md       # Korean guidelines
â”‚   â””â”€â”€ project-guidelines.pdf # Original guidelines (PDF)
â”‚
â”œâ”€â”€ models/                     # Saved trained models
â”‚   â”œâ”€â”€ scaler_full.pkl        # Feature scaler (Full dataset)
â”‚   â””â”€â”€ scaler_objective.pkl   # Feature scaler (Objective dataset)
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks (analysis workflow)
â”‚   â”œâ”€â”€ 01_data_understanding.ipynb
â”‚   â”œâ”€â”€ 02_exploratory_data_analysis.ipynb
â”‚   â”œâ”€â”€ 03_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 04_modeling_and_evaluation.ipynb
â”‚   â”œâ”€â”€ ANALYSIS_01_DATA_UNDERSTANDING.md
â”‚   â”œâ”€â”€ ANALYSIS_02_EXPLORATORY_DATA_ANALYSIS.md
â”‚   â”œâ”€â”€ ANALYSIS_03_DATA_PREPROCESSING.md
â”‚   â””â”€â”€ ANALYSIS_04_MODELING_EVALUATION.md
â”‚
â”œâ”€â”€ reports/                    # Final reports and documentation
â”‚   â””â”€â”€ FINAL_PROJECT_REPORT.md
â”‚
â”œâ”€â”€ results/                    # Analysis results
â”‚   â”œâ”€â”€ figures/               # Generated plots (DPI 300+)
â”‚   â”‚   â”œâ”€â”€ mos_distribution.png
â”‚   â”‚   â”œâ”€â”€ correlation_heatmap.png
â”‚   â”‚   â”œâ”€â”€ buffering_analysis.png
â”‚   â”‚   â”œâ”€â”€ network_type_analysis.png
â”‚   â”‚   â”œâ”€â”€ demographics_analysis.png
â”‚   â”‚   â”œâ”€â”€ confusion_matrices.png
â”‚   â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”‚   â””â”€â”€ model_comparison.png
â”‚   â”œâ”€â”€ metrics/               # Performance metrics
â”‚   â”‚   â””â”€â”€ model_comparison.csv
â”‚   â””â”€â”€ tables/                # Result tables
â”‚
â”œâ”€â”€ src/                        # Source code (reusable modules)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_loader.py     # Data loading utilities
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model_utils.py     # Model training/evaluation utilities
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â””â”€â”€ plot_utils.py      # Visualization utilities
â”‚   â””â”€â”€ utils/                 # General utilities
â”‚
â”œâ”€â”€ .gitignore                  # Git ignore file
â”œâ”€â”€ README.md                   # Main project overview
â””â”€â”€ requirements.txt            # Python dependencies
```

---

## Setup and Installation

### Prerequisites
- Python 3.8+
- pip package manager

### Installation Steps

1. **Clone or navigate to the project directory**
```bash
# Example (adjust path to your local directory):
cd "path/to/Poqemon-QoE-Dataset-master"
```

2. **Install required packages**
```bash
pip install -r requirements.txt
```

3. **Verify data files**
Check that data files exist in `data/raw/`:
- pokemon.csv
- pokemon.arff
- pokemon.data
- pokemon.names

---

## Usage

### Running the Analysis

Follow the notebooks in order:

1. **Data Understanding** (`01_data_understanding.ipynb`)
   - Load and inspect dataset
   - Understand feature categories
   - Analyze target distribution
   - Identify data quality issues

2. **Exploratory Data Analysis** (`02_exploratory_data_analysis.ipynb`)
   - Feature correlations
   - Network type impact
   - Buffering analysis
   - User demographics patterns

3. **Data Preprocessing** (`03_data_preprocessing.ipynb`)
   - Feature engineering
   - Categorical encoding
   - Train/test split
   - Feature scaling

4. **Modeling and Evaluation** (`04_modeling_and_evaluation.ipynb`)
   - Baseline model
   - Multiple ML algorithms
   - Model comparison
   - Feature importance analysis

### Using Utility Scripts

```python
# Load data
from src.data.data_loader import load_pokemon_data
df = load_pokemon_data()

# Evaluate model
from src.models.model_utils import ModelEvaluator
evaluator = ModelEvaluator(model, 'RandomForest')
results = evaluator.evaluate(X_train, X_test, y_train, y_test)

# Create visualizations
from src.visualization.plot_utils import plot_mos_distribution
plot_mos_distribution(df['MOS'], save_path='results/figures/mos_dist.png')
```

---

## Methodology

### Analysis Approach (Following Guidelines)

This project follows a **storytelling approach** where:

1. **WHY before WHAT**: Every analysis step explains its motivation
2. **Expected vs Actual**: Compare predictions with actual results
3. **Critical Assessment**: Identify limitations and feasibility concerns
4. **Selective Reporting**: Only interesting/useful findings included

### Data Science Protocol

1. **Data Understanding**
   - Dataset characteristics
   - Feature meanings and distributions
   - Class imbalance identification

2. **Exploratory Analysis**
   - Correlation analysis
   - Pattern discovery
   - Hypothesis testing

3. **Preprocessing**
   - Feature selection (remove high-cardinality features)
   - Encoding (one-hot for nominal, keep ordinal)
   - Engineering (buffering severity, network generation, video quality index)
   - Stratified train/test split (80/20)
   - Standard scaling (fit on train only!)

4. **Modeling**
   - Baseline: Majority class predictor
   - Linear: Logistic Regression
   - Non-linear: Decision Tree, Random Forest, Gradient Boosting
   - Evaluation: Accuracy, Precision, Recall, F1, Cohen's Kappa

5. **Critical Evaluation**
   - Real-world feasibility
   - Computational requirements
   - Model limitations
   - Deployment considerations

---

## Key Findings

**Dataset Insights:**
- âœ… Severe class imbalance (MOS=4 represents 50.8% of samples)
- âœ… Data leakage detected in subjective feedback features (QoF_* correlation r=0.84 with MOS)
- âœ… LTE networks show significantly better QoE than EDGE/UMTS
- âœ… Buffering events have strong negative correlation with user satisfaction

**Model Performance:**
- **Best model (Objective)**: Gradient Boosting with 59.5% accuracy
  - Baseline: 50.8% (majority class predictor)
  - Improvement: +8.7 percentage points
- **Best model (Full with data leakage)**: Random Forest with 81.9% accuracy
- **Most important features**: QoF_video, QoF_audio, QoA_BUFFERINGcount, QoS_type, QoA_VLCbitrate

**Limitations:**
- âœ… Model struggles with minority classes (MOS=1,2,3) due to class imbalance
- âœ… Overfitting concerns (Random Forest shows 46.3% overfit gap on Objective features)
- âœ… Real-world deployment limited to objective features only (no user feedback)
- âœ… Modest performance improvement over baseline suggests inherent prediction difficulty

**Recommendations:**
- âœ… Address class imbalance using SMOTE or class weights
- âœ… Collect more samples for minority classes (MOS=1,2,3)
- âœ… Deploy only Objective models in production (no data leakage)
- âœ… Consider ensemble methods or deep learning for improvement
- âœ… Implement real-time QoE monitoring using network metrics

---

## Evaluation Metrics

### Why These Metrics?

- **Accuracy**: Overall correctness (but misleading with imbalance)
- **Precision/Recall/F1**: Better for imbalanced datasets
- **Cohen's Kappa**: Accounts for chance agreement (good for ordinal MOS)
- **MAE/RMSE**: Regression-style metrics (MOS is ordinal)
- **Confusion Matrix**: Shows where model makes mistakes

### Visualization Standards (Following Guidelines)

- **Resolution:** DPI 300+ (publication quality)
- **Fonts:** Minimum 12pt, titles 14pt
- **Colors:**
  - ğŸŸ¢ Green: Excellent (>80%)
  - ğŸŸ¡ Yellow: Good (70-80%)
  - ğŸŸ  Orange: Fair (60-70%)
  - ğŸ”´ Red: Poor (<60%)
- **Clarity:** Clean labels, no raw output, rounded decimals

---

## References

### Papers
1. Lamine Amour, Sami Souihi, Said Hoceini, Abdelhamid Mellouk (2015).
   "Building a Large Dataset for Model-based QoE Prediction in the Mobile Environment."
   ACM MSWiM 2015.

2. StÃ©phanie Moteau, Fabrice Guillemin, Thierry Houdoin (2017).
   "Correlation between QoS and QoE for HTTP YouTube content in Orange cellular networks."

### Dataset Source
- Pokemon Project: Platform Quality Evaluation of Mobile Networks
- Paris Est CrÃ©teil University, France
- Contact: lamine.amour@u-pec.fr

---

## Project Guidelines Compliance

This project follows the guidelines provided by Alessandro Maddaloni (Telecom SudParis):

âœ… **Context**: Dataset and goals clearly explained
âœ… **Data Understanding**: EDA with selective reporting
âœ… **Protocol**: Preprocessing steps justified
âœ… **Analysis Path**: Storytelling approach with "WHY"
âœ… **Expected vs Actual**: Comparisons throughout
âœ… **Critical Assessment**: Limitations acknowledged
âœ… **Clean Reporting**: No raw output, formatted results
âœ… **High-Quality Visuals**: DPI 300+, large fonts, color coding

---

## Next Steps

1. **Run Complete Analysis**
   - Execute all notebooks in order
   - Fill in analysis findings
   - Generate all visualizations

2. **Model Improvement**
   - Address class imbalance (SMOTE, class weights)
   - Hyperparameter tuning (GridSearchCV)
   - Try advanced models (XGBoost, Neural Networks)
   - Feature selection

3. **Report Writing**
   - Compile findings
   - Write comprehensive report
   - Include critical assessment
   - Prepare presentation

4. **Deployment Considerations**
   - Real-time prediction feasibility
   - Model serving architecture
   - Monitoring and retraining strategy

---

## Project Information

**Course**: Data Science - Theory to Practice
**Institution**: Telecom SudParis
**Project Completed**: October 2024
**Last Updated**: November 2024

---

## License

This project is for educational purposes. Dataset source and original authors should be cited in any publication.
